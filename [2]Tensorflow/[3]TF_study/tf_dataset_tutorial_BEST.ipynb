{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.print(tf.__version__)\n",
    "tf.print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Random seed for TF\n",
    "We need to set Random seed to be able reproduce the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(2)\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset, Shuffle, Repeat, Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (4,), types: tf.int32>\n",
      "\n",
      "Output using for loop: \n",
      "-------------------\n",
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\n",
      "tf.Tensor([5 6 7 8], shape=(4,), dtype=int32)\n",
      "tf.Tensor([ 9 10 11 12], shape=(4,), dtype=int32)\n",
      "\n",
      "Output using as_numpy_iterator(): \n",
      "-------------------\n",
      "[array([1, 2, 3, 4]), array([5, 6, 7, 8]), array([ 9, 10, 11, 12])]\n"
     ]
    }
   ],
   "source": [
    "# the simplest way to create dataset from a raw data is use tf.data.Dataset.from_tensor_slices\n",
    "# Also I found that create dataset from generator is very usefull, but right now let's do it with from_tensor_slices method\n",
    "np_arr = np.array([[1,2,3, 4], [5,6,7,8], [9,10,11,12]]) #NOTE: 3 x 4 numpy array\n",
    "#create dataset from numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(np_arr) #NOTE: 3 will be removed as the number of samples and only 4 left which is the size of one sample.\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "print(\"\\nOutput using for loop: \\n-------------------\")\n",
    "#read all data from dataset\n",
    "for element in dataset:\n",
    "    print(element)\n",
    "\n",
    "# or we can use as_numpy_iterator() method to print the complete dataset\n",
    "\n",
    "# as_numpy_iterator - Returns an iterator which converts all elements of the dataset to numpy.\n",
    "print(\"\\nOutput using as_numpy_iterator(): \\n-------------------\")\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "# Tensorflow help says: Use as_numpy_iterator to inspect the content of your dataset. \n",
    "# personally i don't use it often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\n",
      "tf.Tensor([5 6 7 8], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#we don't need to iterate through all dataset, we can use 'take' to take predefine number of elements\n",
    "for element in dataset.take(2):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch\n",
    "Combines consecutive elements of this dataset into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3 4]\n",
      " [5 6 7 8]], shape=(2, 4), dtype=int32)\n",
      "tf.Tensor([[ 9 10 11 12]], shape=(1, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#we will sample our dataset of batches of two elements #NOTE: Each element is of size 4 (components) #NOTE: batch 只看element，不看component\n",
    "dataset_with_batch = dataset.batch(2)\n",
    "\n",
    "for element in dataset_with_batch.take(2):\n",
    "    print(element)\n",
    "\n",
    "#as result we have two tensors one that contains 2 elements (as we defined batch of 2) \n",
    "#and another one contains only one element, because we don't have more data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch(1):\n",
      "----------------\n",
      "tf.Tensor([[1 2 3 4]], shape=(1, 4), dtype=int32)\n",
      "tf.Tensor([[5 6 7 8]], shape=(1, 4), dtype=int32)\n",
      "tf.Tensor([[ 9 10 11 12]], shape=(1, 4), dtype=int32)\n",
      "batch(2):\n",
      "----------------\n",
      "tf.Tensor(\n",
      "[[1 2 3 4]\n",
      " [5 6 7 8]], shape=(2, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#if we want, we can use drop_remainder=True to get only full batches\n",
    "print('batch(1):\\n----------------')\n",
    "dataset_with_batch = dataset.batch(1, drop_remainder=True)\n",
    "for element in dataset_with_batch:\n",
    "    print(element)\n",
    "\n",
    "print('batch(2):\\n----------------')\n",
    "for element in dataset.batch(2, drop_remainder=True).take(2):\n",
    "    print(element)   \n",
    "# only one tensor that contains two elements from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch is an intresting method, with it we can create data for RNN\n",
    "# but right now take a look at the shape of our new tensor: batch(2) => (2, 4)\n",
    "# original data shape: shape=(4,) \n",
    "# batch(1) data shape: shape=(1, 4)\n",
    "# batch(2) data shape: shape=(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1 2 3 4]\n",
      "  [5 6 7 8]]], shape=(1, 2, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# and sure we can use batch many times\n",
    "for element in dataset.batch(2, drop_remainder=True).batch(1).take(1):\n",
    "    print(element)  \n",
    "\n",
    "#have a look at the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unbatch\n",
    "Splits elements of a dataset into multiple elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      "-----------------\n",
      "tf.Tensor(\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]], shape=(3, 4), dtype=int32)\n",
      "\n",
      "Unbatched dataset:\n",
      "-----------------\n",
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\n",
      "tf.Tensor([5 6 7 8], shape=(4,), dtype=int32)\n",
      "tf.Tensor([ 9 10 11 12], shape=(4,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\n",
      "tf.Tensor([5 6 7 8], shape=(4,), dtype=int32)\n",
      "tf.Tensor([ 9 10 11 12], shape=(4,), dtype=int32)\n",
      "\n",
      "Unbatched dataset many times:\n",
      "-----------------\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# we can reduce our shape using this method\n",
    "print(\"Original dataset:\\n-----------------\")\n",
    "dataset_batch = dataset.repeat(2).batch(3) #NOTE: repeat 2 means the dataset has 6 elements and 4 components each elements. #NOTE take 3 elements as a batch\n",
    "for element in dataset_batch:               #NOTE: Therefore, there are only 2 batches.\n",
    "    print(element)\n",
    "# assume that our original dataset consists of two tensors each of shape = (3,4)\n",
    "# but we don't want that shape and want to have six tensors instead\n",
    "print(\"\\nUnbatched dataset:\\n-----------------\")\n",
    "dataset_unbatch = dataset_batch.unbatch()\n",
    "for element in dataset_unbatch:\n",
    "    print(element)\n",
    "\n",
    "# Actually we can do it so many times until our Tensors will not have shape = () / constant\n",
    "print(\"\\nUnbatched dataset many times:\\n-----------------\")\n",
    "dataset_unbatch = dataset_batch.unbatch().unbatch() #HACK: 这是绝对的HACK，多次unbatch()将高纬度的数据降至低纬度\n",
    "for element in dataset_unbatch:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat\n",
    "Repeats this dataset so each original value is seen count times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of original dataset: 3\n",
      "size of dataset with repeat(0): 0\n",
      "size of dataset with repeat(1): 3\n",
      "size of dataset with repeat(2): 6\n",
      "size of dataset with repeat(): -1\n",
      "is INFINITE_CARDINALITY: True\n",
      "dataset.__len__(): 3\n"
     ]
    }
   ],
   "source": [
    "# repeat will repeat your dataset \n",
    "# normally you will always start from 2, because the parameter defines how many dataset you want to have, \n",
    "# if you set 0 you will get empty dataset, if you set 1 you will get your original dataset, if you set 2 you will have original and one copy, and so on. \n",
    "# if you leave the parameter empty you will have infinite dataset\n",
    "dataset_with_repeat = dataset.repeat(2) #NOTE: two datasets: one is original, one is copy, altogether 6 elements!\n",
    "#! cardinality is the number of elements in the dataset!\n",
    "# to identify the length of your data, we can use #!cardinality\n",
    "# Returns the cardinality of the dataset, if known.\n",
    "tf.print(\"size of original dataset:\", dataset.cardinality()) #* 3 elements\n",
    "tf.print(\"size of dataset with repeat(0):\", dataset.repeat(0).cardinality()) #* empty dataset due to '0' input argument\n",
    "tf.print(\"size of dataset with repeat(1):\", dataset.repeat(1).cardinality()) #* 3 elements \n",
    "tf.print(\"size of dataset with repeat(2):\", dataset.repeat(2).cardinality()) #* 6 elements\n",
    "tf.print(\"size of dataset with repeat():\", dataset.repeat().cardinality()) #* infinite elements returns -1\n",
    "tf.print(\"is INFINITE_CARDINALITY:\", \"True\" if dataset.repeat().cardinality()==tf.data.INFINITE_CARDINALITY else \"False\")\n",
    "#it is also possible to receive tf.data.UNKNOWN_CARDINALITY if the length could not be determine  #!(e.g. when the dataset source is a file).\n",
    "\n",
    "# dataset has also method __len__() that can be used to get length of dataset #NOTE: the number of elements\n",
    "tf.print(\"dataset.__len__():\", dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: [array([1, 2, 3]), array([4, 5, 6]), array([7, 8, 9])]\n",
      "dataset_with_repeat: [array([1, 2, 3]), array([4, 5, 6]), array([7, 8, 9]), array([1, 2, 3]), array([4, 5, 6]), array([7, 8, 9])]\n"
     ]
    }
   ],
   "source": [
    "# and as our dataset is a small we can use a simple way to see all elements in dataset\n",
    "print(\"dataset:\", list(dataset.as_numpy_iterator())) # put all the elments into one list\n",
    "print(\"dataset_with_repeat:\", list(dataset_with_repeat.as_numpy_iterator())) # put all the elments into one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3 4]\n",
      " [5 6 7 8]], shape=(2, 4), dtype=int32)\n",
      "tf.Tensor([[ 9 10 11 12]], shape=(1, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 2 3 4]\n",
      " [5 6 7 8]], shape=(2, 4), dtype=int32)\n",
      "tf.Tensor([[ 9 10 11 12]], shape=(1, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#now we can try to use batch + repeat together\n",
    "#in the case you want to repeat your dataset after batching you place repeat at the end\n",
    "dataset_b_r = dataset.batch(2).repeat(2)\n",
    "# you you can use it like this\n",
    "dataset_b_r = dataset.batch(2)\n",
    "dataset_b_r = dataset_b_r.repeat(2)\n",
    "\n",
    "for e in dataset_b_r:\n",
    "    print(e)\n",
    "#NOTE: the result is for tensors, two full tensors and two tensors with one element, because we repeat after batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3 4]\n",
      " [5 6 7 8]], shape=(2, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 9 10 11 12]\n",
      " [ 1  2  3  4]], shape=(2, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 5  6  7  8]\n",
      " [ 9 10 11 12]], shape=(2, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#in the case you want to have more elements in dataset and then create batches\n",
    "dataset_b_r = dataset.repeat(2).batch(2)\n",
    "for e in dataset_b_r:\n",
    "    print(e)\n",
    "#as you may see, we get three tensors each of two elements\n",
    "#it could be usefull if we do not care of the element order, because in the second tensor we have last and first elements from our dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffles\n",
    "Randomly shuffles the elements of this dataset\n",
    "\n",
    "From Tensorflow help:\n",
    "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
    "\n",
    "#### For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:             [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "dataset.shuffle(2):  [0, 1, 2, 3, 5, 6, 4, 8, 7, 9]\n",
      "dataset.shuffle(10): [6, 3, 2, 4, 9, 0, 1, 7, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "# we can see that if we set shuffle buffer to 2 our first element always will be 0 or 1, because only first two elements \n",
    "# only two elements will be taken from dataset, then when one of these elements selected and taken from the shuffle buffer \n",
    "# next element (2) from the dataset will be taken. So in the second place we can have only, 0,1 or 2 and so on. \n",
    "ds = tf.data.Dataset.range(10)\n",
    "print(\"dataset:            \", list(ds.as_numpy_iterator()))\n",
    "ds = ds.shuffle(2)\n",
    "print(\"dataset.shuffle(2): \", list(ds.as_numpy_iterator()))\n",
    "ds = ds.shuffle(10)\n",
    "print(\"dataset.shuffle(10):\", list(ds.as_numpy_iterator()))\n",
    "# But if we set shuffle bufffer to 10 that all our dataframe fits in this buffer, we can see that our new dataframe is fully randomly shuffled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\n",
      "tf.Tensor([ 9 10 11 12], shape=(4,), dtype=int32)\n",
      "tf.Tensor([5 6 7 8], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Randomly shuffles the elements of this dataset\n",
    "dataset_shfls = dataset.shuffle(3)\n",
    "\n",
    "for e in dataset_shfls:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 9 10 11 12]\n",
      " [ 5  6  7  8]\n",
      " [ 1  2  3  4]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 9 10 11 12]\n",
      " [ 5  6  7  8]\n",
      " [ 1  2  3  4]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 9 10 11 12]\n",
      " [ 5  6  7  8]\n",
      " [ 1  2  3  4]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# set reshuffle_each_iteration=False attribute to false will give the same result for #! all repeated dataset\n",
    "# NOTE: What happens is 3 x 4 dataset is shuffled by the element index. \n",
    "# NOTE: If the original index is 0, 1, 2.Then the shuffled index 1,0,2\n",
    "# NOTE: Then the shuffled dataset is repeated 3 times. -> 1,0,2,1,0,2,1,0,2\n",
    "# NOTE: Then, batching the new dataset by grouping every three elements as a batch\n",
    "# NOTE: Eventually, the result will be 1st batch [1,0,2] , 2nd batch [1,0,2], 3rd batch [1,0,2]. Hence, three same batches.\n",
    "dataset_all = dataset.shuffle(3, reshuffle_each_iteration=False).repeat(3).batch(3) \n",
    "for e in dataset_all:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 9 10 11 12]\n",
      " [ 5  6  7  8]\n",
      " [ 1  2  3  4]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 5  6  7  8]\n",
      " [ 1  2  3  4]\n",
      " [ 9 10 11 12]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 9 10 11 12]\n",
      " [ 1  2  3  4]\n",
      " [ 5  6  7  8]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# #! set reshuffle_each_iteration=True will give the different result for next repeated dataset\n",
    "# NOTE: What happens is 3 x 4 dataset is shuffled by the element index. \n",
    "# NOTE: If the original index is 0, 1, 2.Then the first shuffled index 2,1,0\n",
    "# NOTE: Then the dataset is repeated once. The repeated dataset is reshuffled as 1,0,2\n",
    "# NOTE: Then the dataset is repeated again. The repeated dataset is reshuffled as 2,0,1\n",
    "# NOTE: Then, batching the new dataset by grouping every three elements as a batch\n",
    "# NOTE: Eventually, the result will be 1st batch [2,1,0] , 2nd batch [1,0,2], 3rd batch [2,0,1]. Hence, three different batches.\n",
    "dataset_all = dataset.shuffle(3, reshuffle_each_iteration=True).repeat(3).batch(3)\n",
    "for e in dataset_all:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply, Map\n",
    "These methods are very usuful when you want to perform some transformation of your dataset (`create new features, normalization, scaling and etc.`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "Maps map_func across the elements of this dataset.\n",
    "\n",
    "Map works on elements, Apply works on dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call from function f1\n",
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\n",
      "call from function f1\n",
      "tf.Tensor([5 6 7 8], shape=(4,), dtype=int32)\n",
      "call from function f1\n",
      "tf.Tensor([ 9 10 11 12], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#Tipp: use tf.print for debug (if you use print instead of tf.print you will see the message only once)\n",
    "def map_func(x):\n",
    "    tf.print(\"call from function f1\")\n",
    "    return x\n",
    "\n",
    "dataset_map = dataset.map(map_func) #* Work on every element in the dataset!\n",
    "for e in dataset_map:\n",
    "    print(e)\n",
    "\n",
    "# you can see that the message is printed three time, on each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset    : [array([1, 2, 3, 4]), array([5, 6, 7, 8]), array([ 9, 10, 11, 12])]\n",
      "dataset_map: [array([2, 4, 1, 3]), array([8, 5, 7, 6]), array([12,  9, 10, 11])]\n"
     ]
    }
   ],
   "source": [
    "# Let's create a function that randomly shuffles the values in elements (columns)\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def f1(x):\n",
    "    idx = tf.random.shuffle(tf.range(tf.shape(x)[0]) )  #NOTE: the shape is (4,0)\n",
    "    return tf.gather(x, idx, axis=0) #NOTE: gather() just grab the data corresponding to the index of idx from x. !! The index could be a list with shuffled order.\n",
    "\n",
    "dataset_map = dataset.map(f1)\n",
    "\n",
    "print(\"dataset    :\", list(dataset.as_numpy_iterator()))\n",
    "print(\"dataset_map:\", list(dataset_map.as_numpy_iterator()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2  4  1  3]\n",
      " [ 8  5  7  6]\n",
      " [12  9 10 11]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  3  4  2]\n",
      " [ 7  5  8  6]\n",
      " [11 10 12  9]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# #* we can use in any place of pipeline\n",
    "dataset_map = dataset.repeat(2).map(f1).batch(3)\n",
    "#execute\n",
    "for e in dataset_map:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 9 10 11 12]\n",
      " [ 1  2  3  4]\n",
      " [ 5  6  7  8]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 9 10 11 12]\n",
      " [ 1  2  3  4]\n",
      " [ 5  6  7  8]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#intresting, if you will put batch before map, it looks like the random generator will be reinitialized\n",
    "dataset_map = dataset.repeat(2).batch(3).map(f1)\n",
    "#execute\n",
    "for e in dataset_map:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply\n",
    "Applies a transformation function to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some_func: [array([1, 2, 3, 4]), array([5, 6, 7, 8]), array([ 9, 10, 11, 12])]\n",
      "---------------\n",
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\n",
      "tf.Tensor([5 6 7 8], shape=(4,), dtype=int32)\n",
      "tf.Tensor([ 9 10 11 12], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Here is very important to remember that #!Apply method works on the whole dataset!\n",
    "\n",
    "# first we need to define transformation function. It could be any function that #!return a transformed dataset\n",
    "# our function just prints the dataset\n",
    "def some_func(ds):\n",
    "    tf.print(\"some_func:\",list(ds.as_numpy_iterator()))\n",
    "    tf.print(\"---------------\")\n",
    "    return (ds)\n",
    "\n",
    "dataset_apply = dataset.apply(some_func)\n",
    "for e in dataset_apply:\n",
    "    print(e)\n",
    "\n",
    "#the some_func #!called only once, it applies to complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.         0.09090909 0.18181819 0.27272728], shape=(4,), dtype=float32)\n",
      "tf.Tensor([0.36363637 0.45454547 0.54545456 0.6363636 ], shape=(4,), dtype=float32)\n",
      "tf.Tensor([0.72727275 0.8181818  0.90909094 1.        ], shape=(4,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2147483648"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this function is useful in case you have a very big dataset and you want to perform a transformation \n",
    "# on a complete dataset for example we want to calculate the \n",
    "\n",
    "def some_func(ds):\n",
    "\n",
    "    #scale all values from 0 to 1\n",
    "    def prep_scale(data, x_min, x_max):\n",
    "        data = tf.cast(data, tf.float32)\n",
    "        x_max = tf.cast(x_max, tf.float32)\n",
    "        x_min = tf.cast(x_min, tf.float32)\n",
    "        data = (data-x_min)/(x_max-x_min + 0.000000001)\n",
    "        return data\n",
    "\n",
    "    #find min value in tensor\n",
    "    def r_min(c, x): #! c is the state, and x is the value\n",
    "        k = tf.reduce_min(x)\n",
    "        if c < k:\n",
    "            k = c\n",
    "        return k #! the final state\n",
    "\n",
    "    #find max value in tensor\n",
    "    def r_max(c, x):\n",
    "        k = tf.reduce_max(x)\n",
    "        if c > k:\n",
    "            k = c\n",
    "        return k\n",
    "\n",
    "    #need to remember that here we are working with dataset not with tensor\n",
    "    x_min = ds.reduce(tf.int32.max, r_min) #! reduce(np.int64(0), lambda state, value: state + value).numpy(), everytime, state + value = new state. The result is the final state\n",
    "    x_max = ds.reduce(tf.int32.min, r_max) #!\n",
    "    #then we can use map function to perform transformation on each element\n",
    "    ds = ds.map(lambda x: prep_scale(x, x_min, x_max)) \n",
    "    #NOTE: Since there are three input arguments, so did not use \"ds = ds.map(prep_scale(x, x_min, x_max))\" which is wrong. \n",
    "    #NOTE: I guess \"ds=ds.map(prep_scale)\" is the right way to use the map() method.\n",
    "    return ds\n",
    "\n",
    "dataset_apply = dataset.apply(some_func)\n",
    "for e in dataset_apply:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "Filters this dataset according to predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [4 5 6]], shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#create a flter to select all elements where the first element of array is less then 5\n",
    "def filter_func(x):\n",
    "    #tf.print('value in data set',x)\n",
    "    k = tf.gather(x, 0, axis=0)\n",
    "    return k < 5\n",
    "\n",
    "dataset_filter = dataset.filter(filter_func).batch(2)\n",
    "for e in dataset_filter:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concantinate and zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concantinate\n",
    "Creates a Dataset by concatenating the given dataset with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4, 5, 6, 7]\n",
      "[array([1, 2], dtype=int64),\n",
      " array([3, 4], dtype=int64),\n",
      " array([4, 5], dtype=int64),\n",
      " array([6, 7], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "# i will take just an example from tensorflow page, because it is very simple method, but very useful\n",
    "a = tf.data.Dataset.range(1, 3) # ==> [ 1, 2, 3 ]\n",
    "b = tf.data.Dataset.range(4, 8,)  # ==> [ 4, 5, 6, 7 ]\n",
    "ds = a.concatenate(b)\n",
    "tf.print(list(ds.as_numpy_iterator()))\n",
    "\n",
    "# or like this\n",
    "a = tf.data.Dataset.range(1, 5).batch(2)\n",
    "b = tf.data.Dataset.range(4, 8).batch(2) \n",
    "ds = a.concatenate(b)\n",
    "tf.print(list(ds.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip\n",
    "Creates a Dataset by zipping together the given datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (2, 5), (3, 6), (4, 7)]\n",
      "[(array([1, 2], dtype=int64), array([4, 5], dtype=int64)),\n",
      " (array([3, 4], dtype=int64), array([6, 7], dtype=int64))]\n"
     ]
    }
   ],
   "source": [
    "# it is the same as in python\n",
    "a = tf.data.Dataset.range(1, 5)  # ==> [ 1, 2, 3, 4 ]\n",
    "b = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\n",
    "ds = tf.data.Dataset.zip((a, b)) #! zip two element into a tuple\n",
    "\n",
    "tf.print(list(ds.as_numpy_iterator()))\n",
    "\n",
    "a = tf.data.Dataset.range(1, 5).batch(2)  # ==> [ 1, 2, 3, 4 ]\n",
    "b = tf.data.Dataset.range(4, 8).batch(2)  # ==> [ 4, 5, 6, 7 ]\n",
    "ds = tf.data.Dataset.zip((a, b)) #! zip two batches into a tuple\n",
    "\n",
    "tf.print(list(ds.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in function: [1 2] [4 5]\n",
      "in function: [3 4] [6 7]\n",
      "[(array([1, 2], dtype=int64), array([4, 5], dtype=int64)),\n",
      " (array([3, 4], dtype=int64), array([6, 7], dtype=int64))]\n"
     ]
    }
   ],
   "source": [
    "# if you want to process data over zipped dataset, you have to define function with arguments for all units\n",
    "# in our case it should be 2\n",
    "def map_on_zip(x, y):\n",
    "    tf.print(\"in function:\", x, y)\n",
    "    return (x,y)\n",
    "\n",
    "ds_new = ds.map(map_on_zip)\n",
    "tf.print(list(ds_new.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:\n",
      " [(array([1, 2], dtype=int64), array([4, 5], dtype=int64)),\n",
      " (array([3, 4], dtype=int64), array([6, 7], dtype=int64))] \n",
      "original size\n",
      " 2\n",
      "New:\n",
      " [array([5, 7], dtype=int64), array([ 9, 11], dtype=int64)] \n",
      "new size\n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "# lets create a function that will return new dataset with sum of x and y\n",
    "def map_on_zip(x, y):\n",
    "    return x+y\n",
    "\n",
    "ds_new = ds.map(map_on_zip)\n",
    "tf.print(\"original:\\n\", list(ds.as_numpy_iterator()), \"\\noriginal size\\n\",ds.cardinality())\n",
    "tf.print(\"New:\\n\", list(ds_new.as_numpy_iterator()), \"\\nnew size\\n\",ds_new.cardinality())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows\n",
    "Combines (nests of) input elements into a dataset of (nests of) windows.\n",
    "\n",
    "A \"window\" is a finite dataset of flat elements of size size (or possibly fewer if there are not enough input elements to fill the window and drop_remainder evaluates to False).\n",
    "\n",
    "The shift argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window k will be element k * shift of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset.\n",
    "\n",
    "The stride argument determines the stride of the input elements, and the shift argument determines the shift of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[3, 4, 5]\n",
      "[6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "dataset_w = tf.data.Dataset.range(9).window(3, shift=None, stride=1, drop_remainder=False)\n",
    "for window in dataset_w:\n",
    "  print(list(window.as_numpy_iterator()))\n",
    "\n",
    "# so you can that we have new dataset with three new elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[1, 2, 3]\n",
      "[2, 3, 4]\n",
      "[3, 4, 5]\n",
      "[4, 5, 6]\n",
      "[5, 6, 7]\n",
      "[6, 7, 8]\n",
      "[7, 8]\n",
      "[8]\n"
     ]
    }
   ],
   "source": [
    "# if we set shift to 1 and stride to 1 we will have new elements with shift to 1\n",
    "dataset_w = tf.data.Dataset.range(9).window(3, shift=1, stride=1, drop_remainder=False)\n",
    "for window in dataset_w:\n",
    "  print(list(window.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 3]), array([4, 5, 6])]\n",
      "[array([4, 5, 6]), array([7, 8, 9])]\n"
     ]
    }
   ],
   "source": [
    "dataset_w = dataset.window(2, shift=1, stride=1, drop_remainder=True)\n",
    "for window in dataset_w:\n",
    "  print(list(window.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=2>"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window example\n",
    "\n",
    "Using the Window method it is very simple to prepare data for rnn networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length: 8\n",
      "[array([1, 2]),\n",
      " array([2, 5]),\n",
      " array([3, 8]),\n",
      " array([ 4, 12]),\n",
      " array([ 5, 22]),\n",
      " array([ 6, 23]),\n",
      " array([ 7, 24]),\n",
      " array([ 8, 25])]\n"
     ]
    }
   ],
   "source": [
    "#assume that we have elemets with two features\n",
    "a = np.array([[1,2],[2,5],[3,8],[4,12],[5,22],[6,23],[7,24],[8,25]])\n",
    "#create a dataset\n",
    "ds = tf.data.Dataset.from_tensor_slices(a)\n",
    "\n",
    "tf.print(\"dataset length:\",ds.__len__())\n",
    "tf.print(list(ds.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.  0. ]\n",
      " [1.  1.5]\n",
      " [0.5 0.6]], shape=(3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.         0.        ]\n",
      " [0.5        0.6       ]\n",
      " [0.33333334 0.5       ]], shape=(3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.         0.        ]\n",
      " [0.33333334 0.5       ]\n",
      " [0.25       0.8333333 ]], shape=(3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.         0.        ]\n",
      " [0.25       0.8333333 ]\n",
      " [0.2        0.04545455]], shape=(3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.         0.        ]\n",
      " [0.2        0.04545455]\n",
      " [0.16666667 0.04347826]], shape=(3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.         0.        ]\n",
      " [0.16666667 0.04347826]\n",
      " [0.14285715 0.04166667]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# let's create a function that create a new dataset with element of the sequence. Let the sequence length will be equal to 3. \n",
    "# Data transformation: we will calculate percentage change in each element(sequence) over the columns.\n",
    "def new_ds(ds, window_size):\n",
    "    # preprocessing function for calculation percentege over the columns \n",
    "    def prep_pct_chg(data):\n",
    "        data = tf.transpose(data)\n",
    "        data = tf.experimental.numpy.diff(data)/data[:,:-1]\n",
    "        data = tf.transpose(data)\n",
    "        data = tf.cast(data, tf.float32)\n",
    "        data = tf.concat([tf.zeros([1,tf.shape(data)[1]]), data], axis=0)\n",
    "        return data\n",
    "\n",
    "    #function that just print elements\n",
    "    def map_x(x):\n",
    "        tf.print(x)\n",
    "        return (x)\n",
    "\n",
    "    ds = ds.window(window_size, shift=1, stride=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda x: x.batch(window_size))\n",
    "    #ds = ds.map(map_x)\n",
    "    ds = ds.map(prep_pct_chg)\n",
    "    return ds\n",
    "\n",
    "#create dataset\n",
    "ds = tf.data.Dataset.from_tensor_slices(a)\n",
    "#apply function with parameters window_size=3\n",
    "ds = ds.apply(lambda x: new_ds(x, 3))\n",
    "\n",
    "for element in ds:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the same preprocessing function we can use with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>59.3125</td>\n",
       "      <td>56.00000</td>\n",
       "      <td>58.68750</td>\n",
       "      <td>58.28125</td>\n",
       "      <td>53228400.0</td>\n",
       "      <td>37.017384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>58.5625</td>\n",
       "      <td>56.12500</td>\n",
       "      <td>56.78125</td>\n",
       "      <td>56.31250</td>\n",
       "      <td>54119000.0</td>\n",
       "      <td>35.766914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>58.1875</td>\n",
       "      <td>54.68750</td>\n",
       "      <td>55.56250</td>\n",
       "      <td>56.90625</td>\n",
       "      <td>64059600.0</td>\n",
       "      <td>36.144032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>56.9375</td>\n",
       "      <td>54.18750</td>\n",
       "      <td>56.09375</td>\n",
       "      <td>55.00000</td>\n",
       "      <td>54976600.0</td>\n",
       "      <td>34.933285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>56.1250</td>\n",
       "      <td>53.65625</td>\n",
       "      <td>54.31250</td>\n",
       "      <td>55.71875</td>\n",
       "      <td>62013600.0</td>\n",
       "      <td>35.389793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               High       Low      Open     Close      Volume  Adj Close\n",
       "Date                                                                    \n",
       "2000-01-03  59.3125  56.00000  58.68750  58.28125  53228400.0  37.017384\n",
       "2000-01-04  58.5625  56.12500  56.78125  56.31250  54119000.0  35.766914\n",
       "2000-01-05  58.1875  54.68750  55.56250  56.90625  64059600.0  36.144032\n",
       "2000-01-06  56.9375  54.18750  56.09375  55.00000  54976600.0  34.933285\n",
       "2000-01-07  56.1250  53.65625  54.31250  55.71875  62013600.0  35.389793"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "from datetime import datetime\n",
    "ticker = 'MSFT'\n",
    "\n",
    "history_range = {'start': datetime(2000, 1, 1), \n",
    "                 'end': datetime(2006, 12, 31)}\n",
    "\n",
    "stock = pdr.DataReader(ticker, \n",
    "                start=history_range['start'],\n",
    "                end=history_range['end'],\n",
    "                data_source='yahoo')\n",
    "stock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59.3125 56 58.6875 58.28125 53228400 37.017383575439453]\n",
      "[58.5625 56.125 56.78125 56.3125 54119000 35.766914367675781]\n",
      "[58.1875 54.6875 55.5625 56.90625 64059600 36.1440315246582]\n",
      "[56.9375 54.1875 56.09375 55 54976600 34.933284759521484]\n",
      "[56.125 53.65625 54.3125 55.71875 62013600 35.389793395996094]\n"
     ]
    }
   ],
   "source": [
    "dataset_stock = tf.data.Dataset.from_tensor_slices(stock)\n",
    "for element in dataset_stock.take(5):\n",
    "    tf.print( element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.01264489  0.00223214 -0.03248136 -0.03378016  0.01673167 -0.03378059]\n",
      " [-0.00640341 -0.02561247 -0.02146395  0.01054384  0.18368042  0.01054374]\n",
      " [-0.02148228 -0.00914286  0.0095613  -0.03349808 -0.14178984 -0.03349783]], shape=(4, 6), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.00640341 -0.02561247 -0.02146395  0.01054384  0.18368042  0.01054374]\n",
      " [-0.02148228 -0.00914286  0.0095613  -0.03349808 -0.14178984 -0.03349783]\n",
      " [-0.01427003 -0.00980392 -0.03175487  0.01306818  0.12799992  0.01306801]], shape=(4, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# to pass arguments to a function, we can use partial instead of lambda\n",
    "from functools import partial\n",
    "dataset_stock_tf = dataset_stock.apply(partial(new_ds, window_size=4))\n",
    "\n",
    "for element in dataset_stock_tf.take(2):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "990abf919410e96c3129c8ffa9aadc5e7589bde03c7cf40f40d94cfa70602d07"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('tf_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
