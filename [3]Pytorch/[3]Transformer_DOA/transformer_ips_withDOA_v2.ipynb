{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the model\n",
        "----------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "from sklearn import preprocessing\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, \n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout) #NOTE d_model is the embedding size\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        # self.encoder = nn.Embedding(ntoken, d_model) # NOTE: Do not need embedding for IPS, the data itself has 384 dimensional data\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "        self.doa_embedding = DOAEncoding(d_model)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        # self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_doa:Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size] #! Make the src to be a tuple, (csi, doa)\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        # src = self.encoder(src) * math.sqrt(self.d_model) #! the input is 35(just consider it to be # of batches) by 20, the output is 35 by 20 by 200. The embedding turns the indices into vectors of size 200.\n",
        "        \n",
        "        # note: with DOA         \n",
        "        src_doa = self.doa_embedding(src_doa) #!HACK: NOTE: DOA embedding should be in from of the pos encoding !!\n",
        "        # print('src and src_doa shape', src.shape, src_doa.shape)\n",
        "        src = src + src_doa\n",
        "        # src = src_doa\n",
        "\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask) #! the output is the output of the final fully-connected layer of 200 dimension. The dimension here is still the same as 35 by 20 by 200\n",
        "        \n",
        "        output = self.decoder(output) #! The linear layer in the decoder maps the input from 35 by 20 by 200 to 35 by 20 by ntoken (好像是两万多)\n",
        "        return output\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000): # max_len means the maximum time steps or word length\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout) # do not understand why you need dropout here\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)) #1/10000^(2i/dim_model)) #! exp(ln(x))=x, therefore, exp(ln(1/10000^(2i/dim_model)))) = exp(2i/dim_model)*(-ln(10000))\n",
        "        pe = torch.zeros(max_len, 1, d_model) #NOTE: Row always means the \n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term) #PE(pos, 2i) = sin(pos/10000^(2i/dim_model))\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term) #PE(pos, 2i) = cos(pos/10000^(2i/dim_model))\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DOAEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.5): # max_len means the maximum time steps or word length\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout) # do not understand why you need dropout here\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=512, # zzf: this value depends on the unique value of the DoA\n",
        "                               embedding_dim=d_model)\n",
        "                            #    padding_idx=1)\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = self.embedding_layer(x)\n",
        "        # return self.embedding_layer(x)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load and batch data\n",
        "-------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import scipy.io as sio\n",
        "from scipy.io import loadmat, savemat\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange,reduce,repeat\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### load training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataPath = './matlab_input_data/2/features_1d_10snr' #\n",
        "# labelPath = 'label_10cm'\n",
        "input_data_train = sio.loadmat(dataPath)\n",
        "input_data_train = torch.from_numpy(input_data_train['features'])\n",
        "# labels = sio.loadmat(labelPath)\n",
        "# labels = torch.from_numpy(labels['label'] )\n",
        "input_data_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Shuffle the data set since all the samples are arranged in the order where SNR being -10 to 20 sequentially. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input_data_reshape = rearrange(input_data_train, 'a b c d e-> d a b c e')\n",
        "# print(input_data_reshape.shape)\n",
        "# rng = np.random.default_rng()\n",
        "# input_data_shuffle = rng.permuted(input_data_reshape,axis=0) # axis = 0, shuffle the lowest dimension. \n",
        "# print('shuffled data size is', input_data_shuffle.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### preprocessing training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_trajectories = 49\n",
        "n_pts_trajectory = 79\n",
        "# Change the original data dimension [3871, 4, 4, 24] to [79 x 49 x 4 x 4 x 24] \n",
        "## NOTE: The following code is for training dataset of multiple SNR case\n",
        "# input_data_reshape = rearrange(torch.from_numpy(input_data_shuffle), 'a b c d e-> e a d c b')\n",
        "## NOTE: The following code is for training dataset of single SNR case\n",
        "input_train_data_reshape = rearrange((input_data_train), 'a b c d-> d (c b a)')\n",
        "print('input_train_data_reshape',input_train_data_reshape.shape)\n",
        "\n",
        "#NOTE: Scaling: make the mean of each dimension (768 dimensions) to be 0, and the standard deviation to be 1\n",
        "input_train_data_reshape = rearrange((input_train_data_reshape), 'a b-> b a')\n",
        "scaler = preprocessing.StandardScaler()\n",
        "# scaler = preprocessing.MinMaxScaler() # feature_range=(-1,1)\n",
        "train_data_scale = scaler.fit_transform(input_train_data_reshape)\n",
        "train_data_scale = rearrange((train_data_scale), 'a b-> b a')\n",
        "\n",
        "# print(train_data_scale[10])\n",
        "# print(np.mean(train_data_scale, axis=0))\n",
        "# print(np.std(train_data_scale, axis=0))\n",
        "\n",
        "train_data_trajectory = torch.from_numpy(train_data_scale).chunk(n_pts_trajectory) #! divide the data into chunks\n",
        "train_data = torch.stack([item for item in train_data_trajectory]).to(device) # [79 x 49 x 768]\n",
        "train_data = train_data.type(torch.cuda.FloatTensor) #! BUG: Training must use `FloatTensor` type\n",
        "print('training data size',train_data.shape) #note the third dimension is # of SNR from -10 to 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### load DOA data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataPath = './matlab_input_data/2/doa_input_1d_10snr' #\n",
        "input_data_doa = sio.loadmat(dataPath)\n",
        "input_data_doa = torch.from_numpy(input_data_doa['doa_input'])\n",
        "input_data_doa = torch.squeeze(input_data_doa)\n",
        "input_data_doa = input_data_doa.type(torch.cuda.LongTensor) #! HACK: Embedding layer only accepts the LongTensor Type\n",
        "data_doa_chunk = input_data_doa.chunk(n_pts_trajectory) #! divide the data into chunks\n",
        "train_data_doa = torch.stack([item for item in data_doa_chunk]).to(device) # [79 x 49 x 4 x 4 x 24]\n",
        "print('doa data size',train_data_doa.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(train_data_doa.shape)\n",
        "max(train_data_doa[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### load test data (snr = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataPath = './matlab_input_data/features_minus10_to_30snr'\n",
        "# labelPath = 'labels_1d_0snr'\n",
        "input_data_test = sio.loadmat(dataPath)\n",
        "input_data_test = torch.from_numpy(input_data_test['features'])\n",
        "# labels_test = sio.loadmat(labelPath)\n",
        "# labels_test = torch.from_numpy(labels_test['label'] )\n",
        "print('the input data size is',input_data_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Preprocess test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_test_data_reshape = rearrange(input_data_test, 'a b c d e -> e d c b a')\n",
        "\n",
        "data_trajectory_test = input_test_data_reshape.chunk(n_pts_trajectory) #! divide the data into chunks\n",
        "data_stack_test = torch.stack([item for item in data_trajectory_test]) # [79 x 49 x 4 x 4 x 24]\n",
        "print('test data size',data_stack_test.shape)\n",
        "\n",
        "## NOTE: The following code is for training dataset of multiple SNR case\n",
        "test_data = rearrange(data_stack_test, 'a b c d e f-> c a b (d e f)').to(device) #NOTE: the first dimension is # of SNR (-10:30 dB)\n",
        "## NOTE: The following code is for training dataset of single SNR case\n",
        "# input_data_reshape_test = rearrange(input_data_test, 'a b c d -> d c b a')\n",
        "print(f'the size of the test data is {test_data.shape}')\n",
        "\n",
        "# # Creating labels from 0 to 3870"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generate Labels for all positions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels_ips = torch.arange(0,3871)\n",
        "# Change the label dimension from 1d to [79 x 49]\n",
        "labels_ips = labels_ips.reshape(49, 79).t().to(device)\n",
        "print('label shape',labels_ips.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_pts_once = 79 #NOTE: batch dimension N, the length of each batch. In IPS, it means the number of points considered at once\n",
        "def get_batch(source: Tensor, source_doa: Tensor, label: Tensor, i: int) -> Tuple[Tensor, Tensor]: \n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [num_pts, batch_size] NOTE: batch_size = # of trajectories\n",
        "        label: Tensor, shape [num_pts, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [num_pts, batch_size] and\n",
        "        target has shape [num_pts * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(n_pts_once, len(source) - 1 - i)  #! The actual value for i is [0 35 70 105,...]\n",
        "    data = source[i:i+seq_len]\n",
        "    data_doa = source_doa[i:i+seq_len]\n",
        "    target = label[i+1:i+1+seq_len].reshape(-1) #! reshape(-1) will unfold the matrix from the higher dimension to the lower dimension\n",
        "    return data,data_doa,target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initiate an instance\n",
        "--------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ntokens = 3871  # there are 3871 points!!\n",
        "emsize = 768  # 384 # the data sample dimension for each point (similar to embedding dimension here)\n",
        "d_hid = 1024  # dimension of the feedforward network model in nn.TransformerEncoder %NOTE: default is 2048\n",
        "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 4  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.1  # dropout probability % 0.2\n",
        "traj_length = 79\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the model\n",
        "-------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 20.0, gamma=0.95) # after one epoch, the LR becomes 95% of the original LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = test_data[25]\n",
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 10\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(n_pts_once).to(device)\n",
        "    num_batches = traj_length // n_pts_once\n",
        "    \n",
        "    # for    batch, i in enumerate(range(0, train_data.size(0) - 1, n_pts_once)): #(0 35 70 ...) #NOTE: `-1` is for batching the target value\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, 1)): #(0 35 70 ...) #NOTE: `-1` is for batching the target value\n",
        "        data, data_doa, targets = get_batch(train_data,train_data_doa, labels_ips, i) # i = 0, 35,70, ... len(train_data) #! The size of data is 35 by 20, 20 is the batch size\n",
        "        batch_size = data.size(0)\n",
        "\n",
        "        if batch_size != n_pts_once:  # only on last batch\n",
        "            src_mask = src_mask[:batch_size, :batch_size] \n",
        "        output = model(data, data_doa, src_mask) #! The shape of the output is (35, 20, 28782)\n",
        "        loss = criterion(output.view(-1, ntokens), targets) #! out.view(-1,ntokens) will make the shape (35,20,28782) to (700,28782)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        \n",
        "\n",
        "        # total_loss += loss.item()\n",
        "        # if batch % log_interval == 0 and batch > 0:\n",
        "        #     lr = scheduler.get_last_lr()[0]\n",
        "        #     ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "        #     cur_loss = total_loss / log_interval\n",
        "        #     ppl = math.exp(cur_loss)\n",
        "        #     print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "        #             f'lr {lr:02.6f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "        #             f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "        #     total_loss = 0\n",
        "        #     start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor, eval_data_doa:Tensor, labels: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(n_pts_once).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, n_pts_once):\n",
        "            data, data_doa,targets = get_batch(eval_data, eval_data_doa,labels, i)\n",
        "            batch_size = data.size(0)\n",
        "            if batch_size != n_pts_once:\n",
        "                src_mask = src_mask[:batch_size, :batch_size]\n",
        "            output = model(data, data_doa,src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loop over epochs. Save the model if the validation loss is the best\n",
        "we've seen so far. Adjust the learning rate after each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "best_val_loss = float('inf') #float('inf') 1.8\n",
        "epochs = 1000\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    # for snr in range(len(train_data)):\n",
        "    #     train_train_snr = train_data[snr].to(device)\n",
        "    train(model)\n",
        "    # train_train_snr_test = test_data[30].to(device)\n",
        "    val_loss = evaluate(model, train_data,train_data_doa, labels_ips)  # val_data\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    \n",
        "    if epoch % 20 == 0:\n",
        "        print('-' * 89)\n",
        "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "        print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "        lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "        # Save the model \n",
        "        fileName = f'model_numLayer_{nlayers}_numHead_{nhead}_dropout_{dropout}_batchsize_{n_pts_once}_minLoss_{val_loss:.3f}_lr_{lr:.6f}_epoch_{epoch}.pth'\n",
        "        PATH = 'standard_scaling_across_dimensions_model_range_doa_20dBSNR_8antenna_train_on_CSI_and_DOA_2'\n",
        "        if not os.path.exists(PATH):\n",
        "            os.makedirs(PATH)\n",
        "        fullPath = os.path.join(PATH, fileName)\n",
        "        # searchWord = f'model_numLayer_{nlayers}_numHead_{nhead}_dropout_{dropout}_batchsize_{n_pts_once}'\n",
        "        # files = glob.glob(f'model/{searchWord}*.pth')\n",
        "        # for i in files:\n",
        "        #     os.remove(i)\n",
        "        torch.save(best_model.state_dict(), fullPath)\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fileName = f'model_numLayer_{nlayers}_numHead_{nhead}_dropout_{dropout}_batchsize_{n_pts_once}_minLoss_{val_loss:.3f}_lr_{lr:.6f}_epoch_{epoch}.pth'\n",
        "PATH = 'model'\n",
        "if not os.path.exists(PATH):\n",
        "    os.makedirs(PATH)\n",
        "fullPath = os.path.join(PATH, fileName)\n",
        "# searchWord = f'model_numLayer_{nlayers}_numHead_{nhead}_dropout_{dropout}_batchsize_{n_pts_once}'\n",
        "# files = glob.glob(f'model/{searchWord}*.pth')\n",
        "# for i in files:\n",
        "#     os.remove(i)\n",
        "torch.save(best_model.state_dict(), fullPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the model\n",
        "new_model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
        "# new_model.load_state_dict(torch.load('model_doa_singleSNR\\model_numLayer_2_numHead_4_dropout_0.1_batchsize_40_minLoss_0.069_lr_0.000081_epoch_114.pth'))\n",
        "new_model.load_state_dict(torch.load('standard_scaling_across_dimensions_model_range_doa_20dBSNR_8antenna_train_on_CSI_only\\model_numLayer_2_numHead_4_dropout_0.1_batchsize_79_minLoss_0.070_lr_0.000081_epoch_100.pth'))\n",
        "\n",
        "# new_model.load_state_dict(torch.load('model\\model_numLayer_2_numHead_4_dropout_0.1_batchsize_40_minLoss_0.014_lr_0.000060.pth'))\n",
        "new_model.eval() # to turn off the dropout layer .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the best model on the test dataset\n",
        "-------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# test_loss = evaluate(best_model, test_data)\n",
        "# test_ppl = math.exp(test_loss)\n",
        "# print('=' * 89)\n",
        "# print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "#       f'test ppl {test_ppl:8.2f}')\n",
        "# print('=' * 89)\n",
        "def predict1(model: nn.Module, start_point: Tensor, data_length: Tensor, eval_data: Tensor, eval_data_doa:Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(n_pts_once).to(device)\n",
        "    with torch.no_grad():\n",
        "      if data_length > n_pts_once:\n",
        "            raise ValueError(f\"the data_length should not be greater than the batch_size {n_pts_once}\")\n",
        "      else:\n",
        "            #   for i in range(0, eval_data.size(0) - 1, n_pts_once):\n",
        "            # data, targets = get_batch(eval_data, labels, i)\n",
        "            i = start_point\n",
        "            seq_len = min(n_pts_once, len(eval_data) - 1 - i)  #! The actual value for i is [0 n_pts_once 2*n_pts_once ...]\n",
        "            data = eval_data[i:i+seq_len]\n",
        "            # target = label[i+1:i+1+seq_len].reshape(-1) #! reshape(-1) will unfold the matrix from the higher dimension to the lower dimension            \n",
        "            batch_size = data.size(0)\n",
        "            if batch_size != n_pts_once:\n",
        "                  src_mask = src_mask[:batch_size, :batch_size]\n",
        "            output = model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "                  \n",
        "            # total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "    return output_flat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Estimation Error Table !\n",
        "1. First, I need to create a 49 x 79 table.\n",
        "2. 49 is along the x axis, and 79 is along the y axis.\n",
        "3. The spacing between each point is 0.1cm. \n",
        "4. When calculating the error, just need to calculate the distance between the estimated "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_error(table, trajectory_ind, start_point, predicted, spacing):\n",
        "    data_length = len(predicted)\n",
        "    label = table[trajectory_ind][start_point + 1:start_point+data_length + 1]\n",
        "    predicted_ind = predicted.cpu().numpy()\n",
        "    dis_sum = 0\n",
        "    if len(label) != len(predicted):\n",
        "        predicted_ind = predicted_ind[:-1]\n",
        "    # print((predicted_ind), (label))\n",
        "    for x, y in zip(predicted_ind, label):\n",
        "        pred = np.where(table == y) #NOTE the index of the predicted value in the table\n",
        "        grd = np.where(table == x) #NOTE the index of the true label in the table\n",
        "        dis = ((pred[0] - grd[0])**2 + (pred[1] - grd[1])**2)**0.5\n",
        "        \n",
        "        \n",
        "        real_dis = dis * spacing #NOTE: Multiply with the unit\n",
        "        dis_sum = dis_sum + real_dis\n",
        "    ave = dis_sum/data_length # calculate the averaged value\n",
        "    # print(f'the averaged value over {data_length} data points is {ave} cm')\n",
        "    return ave"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Methodology to calculate the error\n",
        "+  First of all, every point should be considered as the starting point once.\n",
        "+ Then for convenient computation, we consider all the locations starting from the start_point all the way to the second to the end.(Should not include the last location because the last one should be predicted by the previous locations)\n",
        "+  Then for each test from an arbitrary starting location, the error is calculated between the correct and incorrect locations. Then calculate the average by dividing the length which is from the starting location to the second location to the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model: nn.Module, start_point: Tensor, data_length: Tensor, eval_data: Tensor, eval_data_doa:Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(n_pts_once).to(device)\n",
        "    with torch.no_grad():\n",
        "      if data_length > n_pts_once:\n",
        "        raise ValueError(f\"the data_length should not be greater than the batch_size {n_pts_once}\")\n",
        "      else:\n",
        "        # if start_point+data_length < 79:\n",
        "        # data = eval_data[start_point:start_point+data_length]\n",
        "        # data_doa = eval_data_doa[start_point:start_point+data_length]\n",
        "        # else:\n",
        "        data = eval_data[start_point:-1] #NOTE:Consider all the remaining locations in the trajectory\n",
        "        data_doa = eval_data_doa[start_point:-1] #NOTE:Consider all the remaining locations in the trajectory\n",
        "        batch_size = data.size(0)\n",
        "        if batch_size != n_pts_once:\n",
        "                src_mask = src_mask[:batch_size, :batch_size]\n",
        "        output = model(data, data_doa,src_mask)\n",
        "        output_flat = output.view(-1, ntokens)\n",
        "    return output_flat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Scenario 1: \n",
        "1. the test data is the same data as the training data.\n",
        "2. the batch size is the same as the training data. (batch_sie <==> n_pts_once)\n",
        "\n",
        "`Result`: The result is 100% accurate if the same data is put into the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trajectory_ind = 0 #NOTE: the index of the trajectory from all 49 trajectories.\n",
        "start_point = 10 # 0, n_pts_once, 2*n_pts_once... #NOTE: the start point in a given trajectory\n",
        "data_length = 12\n",
        "\n",
        "test_input = rearrange(train_data, 'a b c -> b a c') # train_data test_data\n",
        "test_input_doa = rearrange(train_data_doa, 'a b -> b a') # train_data test_data\n",
        "\n",
        "one_trajectory = rearrange(test_input[trajectory_ind], 'a b -> a 1 b') \n",
        "one_trajectory_doa = rearrange(test_input_doa[trajectory_ind], 'a -> a 1') \n",
        "\n",
        "predicted = predict(best_model, start_point, data_length, one_trajectory,one_trajectory_doa) # new_model best_model \n",
        "# test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "# print(predicted.shape)\n",
        "pre_ind = torch.argmax(predicted,axis=1)\n",
        "# print(pre_ind.reshape(n_pts_once,-1))\n",
        "print(f'the predicted locations are {pre_ind.reshape(len(predicted),-1).squeeze()}')\n",
        "# print(labels_ips.)\n",
        "\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Scenario 2: \n",
        "1. the test data is different from the training data. \n",
        "2. For example, the batch_size is 20, one of the 20 data, only the first one is valid, the remaining ones will be 0, just to conform with the batch_size during the training.\n",
        "\n",
        "`Result`: The result is 100% accurate if the same data is put into the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trajectory_ind = 0 #NOTE: the index of the trajectory from all 49 trajectories.\n",
        "start_point = 0 # 0, n_pts_once, 2*n_pts_once... #NOTE: the start point in a given trajectory\n",
        "data_length = 40\n",
        "test_data_one = test_data[2]\n",
        "\n",
        "test_input = rearrange(test_data_one.to(device), 'a b c -> b a c') # train_data test_data\n",
        "print(test_input.shape)\n",
        "\n",
        "test_input_doa = rearrange(train_data_doa.to(device), 'a b -> b a') # train_data test_data\n",
        "print(test_input_doa.shape)\n",
        "\n",
        "# print(test_input.shape)\n",
        "one_trajectory = rearrange(test_input[trajectory_ind], 'a b -> a 1 b') \n",
        "one_trajectory_doa = rearrange(test_input_doa[trajectory_ind], 'a -> a 1') \n",
        "print(one_trajectory.shape)\n",
        "print(one_trajectory_doa.shape)\n",
        "\n",
        "\n",
        "predicted = predict(new_model, start_point, data_length, one_trajectory,one_trajectory_doa)\n",
        "# test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "# print(predicted.shape)\n",
        "pre_ind = torch.argmax(predicted,axis=1)\n",
        "# print(pre_ind.reshape(n_pts_once,-1))\n",
        "print(f'the predicted locations are {pre_ind.reshape(len(predicted),-1).squeeze()}')\n",
        "# print(labels_ips.)\n",
        "\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Scenario 3: \n",
        "Calculate the overall average errors by applying the error reference table\n",
        "NOTE: The average error is calculated across all the positions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualization of the errors in each trajectory as well as the average error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_traj_error(snr, trj_error):\n",
        "    names = [str(i+1) for i in range(len(trj_error))]\n",
        "    values = [i[0] for i in trj_error]\n",
        "    # the average value\n",
        "    ave_error = sum(values)/len(values)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(15, 5))\n",
        "    ax.bar(names,values)\n",
        "    ax.axhline(ave_error, color='k') # horizontal\n",
        "\n",
        "    ax.annotate(\n",
        "        f'Average: {ave_error:.2f} cm',\n",
        "        xy=(27, ave_error), xycoords='data', size=15,\n",
        "        xytext=(-90, 50), textcoords='offset points',\n",
        "        arrowprops=dict(arrowstyle=\"->\",\n",
        "                        connectionstyle=\"arc,angleA=0,armA=50,rad=10\",\n",
        "                        color=\"k\"))\n",
        "    plt.suptitle(f'Positioning Error of Each Trajectory (SNR = {snr}dB)',size=20)\n",
        "    plt.xlabel('# Trajectory',size=15)\n",
        "    plt.ylabel('Error (cm)',size=15)\n",
        "    plt.savefig(f'{snr}snr_train_on_20_snr_with_standard_across_dimensions_scaled_csi_and_doa_0.5dropout.png')\n",
        "    plt.close(fig)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the errors across all the positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "table = np.arange(0, 79 * 49)\n",
        "table = table.reshape(49, 79)\n",
        "spacing = 10 # 10 cm between each point\n",
        "\n",
        "# calculate the overall positioning error\n",
        "data_length = None\n",
        "trj_error = []\n",
        "# error_snr = dict()\n",
        "# NOTE: to test the data generated under all the SNR (from -10dB to 30dB)\n",
        "for snr in range(-10,0):\n",
        "    test_data_snr = test_data[snr+10] # 0 is snr -10 dB, 40 is snr 30 dB, 30 is 20dB, 20 is 10dB 15 is 5 dB 10 is 0dB 5 is -5dB\n",
        "    for trj in range(49): #note: 49 is the number of trajectories\n",
        "        test_input = rearrange(test_data_snr, 'a b c -> b a c') # train_data test_data\n",
        "        test_input_doa = rearrange(train_data_doa.to(device), 'a b -> b a') # train_data test_data\n",
        "\n",
        "        one_trajectory = rearrange(test_input[trj], 'a b -> a 1 b') \n",
        "        one_trajectory_doa = rearrange(test_input_doa[trj], 'a -> a') \n",
        "        all_error = 0\n",
        "        for str_point in range(79-1):  #note: 79 is the number of points in each trajectory\n",
        "\n",
        "            predicted = predict(new_model, str_point, n_pts_once, one_trajectory ,one_trajectory_doa) # new_model\n",
        "              \n",
        "            # predicted = predict2(new_model, str_point, n_pts_once, one_trajectory)\n",
        "            pre_ind = torch.argmax(predicted,axis=1)\n",
        "            # print(len(pre_ind),'b')\n",
        "            err_trj = calculate_error(table, trj, str_point, pre_ind, spacing)\n",
        "            all_error = all_error + err_trj\n",
        "            # print(err_trj)\n",
        "        trj_error.append(all_error/78)\n",
        "    values = [i[0] for i in trj_error]\n",
        "    # the average value\n",
        "    ave_error = sum(values)/len(values)\n",
        "    print(f'SNR = {snr}dB, average error is {ave_error}cm')\n",
        "    error_snr[snr] = ave_error\n",
        "    # plot and save the figure\n",
        "    # plot_traj_error(snr, trj_error)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the errors based on SNR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "error_snr_withDOA = [11.961143134138677,\n",
        "11.961143134138682,\n",
        "11.961143134138682,\n",
        "11.96114313413868,\n",
        "11.961143134138677,\n",
        "11.961143134138675,\n",
        "11.961143134138673,\n",
        "11.96114313413868,\n",
        "11.961117890192812,\n",
        "11.960408324780612,\n",
        "11.913337765814694,\n",
        "11.833442485541887,\n",
        "11.691940741785533,\n",
        "11.508321262775388,\n",
        "11.232074927021413,\n",
        "10.966924682058615,\n",
        "10.758638022697971,\n",
        "10.591753710448339,\n",
        "10.453197031274007,\n",
        "10.336467100197485,\n",
        "10.23971543291192,\n",
        "10.158290129106595,\n",
        "10.089486670090409,\n",
        "10.029971799035957,\n",
        "9.97850032359246,\n",
        "9.934148869108965,\n",
        "9.894463581570045,\n",
        "9.858787049646615,\n",
        "9.826936877837532,\n",
        "9.798474314812347,\n",
        "9.77241820808085,\n",
        "9.748975347481183,\n",
        "9.727286350826676,\n",
        "9.707512706871128,\n",
        "9.68954436590737,\n",
        "9.673289870813424,\n",
        "9.657871267076334,\n",
        "9.643576704507558,\n",
        "9.630197743418245,\n",
        "9.617907146074646,\n",
        "9.606227302757468]\n",
        "error_snr_withoutDOA = [290.7628089254208,\n",
        "290.7628089254208,\n",
        "290.7628089254207,\n",
        "290.7628089254206,\n",
        "290.76280892542064,\n",
        "290.7628089254207,\n",
        "290.76280892542076,\n",
        "290.7628089254208,\n",
        "290.7621112791757,\n",
        "290.56509611148397,\n",
        "290.02624824035615,\n",
        "289.4126485223524,\n",
        "287.81348009725065,\n",
        "287.33056752759063,\n",
        "286.09976396830996,\n",
        "283.52760812115247,\n",
        "280.1168857056598,\n",
        "275.75506138794077,\n",
        "270.81857785664306,\n",
        "265.6840747632065,\n",
        "260.6965537163995,\n",
        "255.97519862106762,\n",
        "251.53136354677372,\n",
        "247.42891935191938,\n",
        "243.65650200764182,\n",
        "240.18452731721783,\n",
        "236.981985620681,\n",
        "234.0360015498321,\n",
        "231.30583584537757,\n",
        "228.7753514949769,\n",
        "226.42295092463726,\n",
        "224.2302487245853,\n",
        "222.1848179598548,\n",
        "220.2768979950747,\n",
        "218.4840351287932,\n",
        "216.80061929851598,\n",
        "215.22224063987625,\n",
        "213.73046910135827,\n",
        "212.32083055845308,\n",
        "210.98531019945162,\n",
        "209.72350357579705]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "error_snr_20 = [265.2960669,\n",
        "265.2960669,\n",
        "265.2960669,\n",
        "265.2960669,\n",
        "265.2960669,\n",
        "265.2960669,\n",
        "265.2960669,\n",
        "265.2960669,\n",
        "265.2261461,\n",
        "264.6108355,\n",
        "270.136278,\n",
        "314.6082377,\n",
        "333.3966625,\n",
        "320.0825991,\n",
        "270.8881627,\n",
        "219.244125,\n",
        "175.6577249,\n",
        "128.269264,\n",
        "90.81045046,\n",
        "60.26615084,\n",
        "38.61689962,\n",
        "23.27780737,\n",
        "13.13905781,\n",
        "7.514934887,\n",
        "4.482336969,\n",
        "2.794334644,\n",
        "1.81388114,\n",
        "1.361596767,\n",
        "1.112289487,\n",
        "0.9582474997,\n",
        "0.9080324687,\n",
        "0.8603478845,\n",
        "0.8407060223,\n",
        "0.7837711584,\n",
        "0.7174819838,\n",
        "0.7121947389,\n",
        "0.6954038997,\n",
        "0.6727326233,\n",
        "0.674547542,\n",
        "0.6607467632,\n",
        "0.659466191]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(list(range(-10,31)),error_snr_withDOA, label='Training with the DOA embedding')\n",
        "plt.plot(list(range(-10,31)),error_snr_withoutDOA,label='Training without the DOA embedding ')\n",
        "# plt.plot(list(range(-10,31)),error_snr_allSNR,label='Training on the data of all SNR ')\n",
        "plt.title(f'Positioning Error with and without DOA embedding (100epochs)',size=20)\n",
        "plt.xlabel('SNR (dB)',size=15)\n",
        "plt.ylabel('Error (cm)',size=15)\n",
        "plt.grid()\n",
        "plt.legend(title_fontsize=100)\n",
        "plt.savefig(f'DOA Embedding.png')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['group_a', 'group_b', 'group_c']\n",
        "values = [1, 10, 100]\n",
        "\n",
        "plt.figure(figsize=(9, 3))\n",
        "\n",
        "plt.bar(names, values)\n",
        "plt.suptitle('Categorical Plotting')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Scenario 4:(DOA only) \n",
        "NOTE: Purpose of this test scenario is to see if the IPS can predict the locations just based on the CSI without the CSI information. \n",
        "+ `Personal guess`: The neural network may quickly converge. However, if the test data and training data are not generated under the same SNR. I think the IPS will be a total disaster. Because as the SNR changes, the DOA actually changes a lot. It is not as stable as the CSI. \n",
        "1. The training data is only the DOA of each location (3871 locations). \n",
        "2. The DOA is integer and is embedded through the embedding layer to 784 dimensions. The data size after embedding is (79,49,784) where 79 is the locations for one trajectory and 49 is the total number of trajectories.\n",
        "3. The test data is the DOA generated under different SNR.\n",
        "+ `Actual Result`: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "table = np.arange(0, 79 * 49)\n",
        "table = table.reshape(49, 79)\n",
        "spacing = 10 # 10 cm between each point\n",
        "snr = 10\n",
        "# calculate the overall positioning error\n",
        "trj_error = []\n",
        "# error_snr = dict()\n",
        "# NOTE: to test the data generated under different SNR ()\n",
        "# for snr in range(1):\n",
        "    # test_data_snr = test_data[snr+10] # 0 is snr -10 dB, 40 is snr 30 dB, 30 is 20dB, 20 is 10dB 15 is 5 dB 10 is 0dB 5 is -5dB\n",
        "test_input = rearrange(train_data, 'a b c -> b a c') # train_data test_data\n",
        "test_input_doa = rearrange(train_data_doa, 'a b -> b a') # train_data test_data\n",
        "for trj in range(49): #note: 49 is the number of trajectories\n",
        "    one_trajectory = rearrange(test_input[trj], 'a b -> a 1 b') \n",
        "    one_trajectory_doa = rearrange(test_input_doa[trj], 'a -> a 1') #! BUG: Here, originally was 'a -> a' which is wrong!!\n",
        "    all_error = 0\n",
        "    for str_point in range(79-1):  #note: 79 is the number of points in each trajectory\n",
        "        n_length = 40\n",
        "        predicted = predict(best_model, str_point, n_length, one_trajectory ,one_trajectory_doa) # new_model\n",
        "        pre_ind = torch.argmax(predicted,axis=1)\n",
        "        err_trj = calculate_error(table, trj, str_point, pre_ind, spacing) #NOTE: The returned value is the average error.\n",
        "        all_error = all_error + err_trj\n",
        "    trj_error.append(all_error/78)\n",
        "values = [i[0] for i in trj_error]\n",
        "\n",
        "# the average value\n",
        "ave_error = sum(values)/len(values)\n",
        "print(f'SNR = {snr}dB, average error is {ave_error}cm, training on doa and CSI data')\n",
        "    # error_snr[snr] = ave_error\n",
        "# plot and save the figure\n",
        "plot_traj_error(snr, trj_error)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
