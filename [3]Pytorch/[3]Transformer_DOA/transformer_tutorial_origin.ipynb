{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Language Modeling with nn.Transformer and TorchText\n",
        "===============================================================\n",
        "\n",
        "This is a tutorial on training a sequence-to-sequence model that uses the\n",
        "`nn.Transformer <https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>`__ module.\n",
        "\n",
        "The PyTorch 1.2 release includes a standard transformer module based on the\n",
        "paper `Attention is All You Need <https://arxiv.org/pdf/1706.03762.pdf>`__.\n",
        "Compared to Recurrent Neural Networks (RNNs), the transformer model has proven\n",
        "to be superior in quality for many sequence-to-sequence tasks while being more\n",
        "parallelizable. The ``nn.Transformer`` module relies entirely on an attention\n",
        "mechanism (implemented as\n",
        "`nn.MultiheadAttention <https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html>`__)\n",
        "to draw global dependencies between input and output. The ``nn.Transformer``\n",
        "module is highly modularized such that a single component (e.g.,\n",
        "`nn.TransformerEncoder <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html>`__)\n",
        "can be easily adapted/composed.\n",
        "\n",
        "![](../_static/img/transformer_architecture.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the model\n",
        "----------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we train a ``nn.TransformerEncoder`` model on a\n",
        "language modeling task. The language modeling task is to assign a\n",
        "probability for the likelihood of a given word (or a sequence of words)\n",
        "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
        "layer first, followed by a positional encoding layer to account for the order\n",
        "of the word (see the next paragraph for more details). The\n",
        "``nn.TransformerEncoder`` consists of multiple layers of\n",
        "`nn.TransformerEncoderLayer <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html>`__.\n",
        "Along with the input sequence, a square attention mask is required because the\n",
        "self-attention layers in ``nn.TransformerEncoder`` are only allowed to attend\n",
        "the earlier positions in the sequence. For the language modeling task, any\n",
        "tokens on the future positions should be masked. To produce a probability\n",
        "distribution over output words, the output of the ``nn.TransformerEncoder``\n",
        "model is passed through a linear layer followed by a log-softmax function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, \n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout) #NOTE d_model is the embedding size\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model) #! the input is 35(just consider it to be # of batches) by 20, the output is 35 by 20 by 200. The embedding turns the indices into vectors of size 200.\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask) #! the output is the output of the final fully-connected layer of 200 dimension. The dimension here is still the same as 35 by 20 by 200\n",
        "        output = self.decoder(output) #! The linear layer in the decoder maps the input from 35 by 20 by 200 to 35 by 20 by ntoken (好像是两万多)\n",
        "        return output\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``PositionalEncoding`` module injects some information about the\n",
        "relative or absolute position of the tokens in the sequence. The\n",
        "positional encodings have the same dimension as the embeddings so that\n",
        "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
        "different frequencies.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000): # max_len means the maximum time steps or word length\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout) # do not understand why you need dropout here\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)) #1/10000^(2i/dim_model)) #! exp(ln(x))=x, therefore, exp(ln(1/10000^(2i/dim_model)))) = exp(2i/dim_model)*(-ln(10000))\n",
        "        pe = torch.zeros(max_len, 1, d_model) #NOTE: Row always means the \n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term) #PE(pos, 2i) = sin(pos/10000^(2i/dim_model))\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term) #PE(pos, 2i) = cos(pos/10000^(2i/dim_model))\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load and batch data\n",
        "-------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial uses ``torchtext`` to generate Wikitext-2 dataset.\n",
        "To access torchtext datasets, please install torchdata following instructions at https://github.com/pytorch/data. \n",
        "\n",
        "The vocab object is built based on the train dataset and is used to numericalize\n",
        "tokens into tensors. Wikitext-2 represents rare tokens as `<unk>`.\n",
        "\n",
        "Given a 1-D vector of sequential data, ``batchify()`` arranges the data\n",
        "into ``batch_size`` columns. If the data does not divide evenly into\n",
        "``batch_size`` columns, then the data is trimmed to fit. For instance, with\n",
        "the alphabet as the data (total length of 26) and ``batch_size=4``, we would\n",
        "divide the alphabet into 4 sequences of length 6:\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "\n",
        "Batching enables more parallelizable processing. However, batching means that\n",
        "the model treats each column independently; for example, the dependence of\n",
        "``G`` and ``F`` can not be learned in the example above.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HYPC300\\anaconda3\\envs\\pt1.11\\lib\\site-packages\\torch\\utils\\data\\datapipes\\utils\\common.py:24: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\HYPC300\\anaconda3\\envs\\pt1.11\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\selecting.py:54: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
            "  warnings.warn(\"Lambda function is not supported for pickle, please use \"\n"
          ]
        }
      ],
      "source": [
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>']) # tokenize the sentences into words. The specail word '<unk>' will be inserted in the start of the vocab.\n",
        "vocab.set_default_index(vocab['<unk>']) # If there is an unknown word that is not included in the vocab, then the default index that corresponds to <unk> will be returned, which is 0. \n",
        "# vocab.get_itos() # List mapping indices to tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter] # vocab(tokens) will return the corresponding indices of the tokens in the generated vocabulary\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter) # put all the corresponding indices of all the words in the training dataset into one tensor list\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2049990])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## test (to delate)！\n",
        "# for num, item in enumerate(train_iter):\n",
        "#     print(item)\n",
        "#     tok = tokenizer(item)\n",
        "#     print(tok)\n",
        "#     inddd = vocab(tok)\n",
        "#     print(inddd)\n",
        "#     voc\n",
        "#     print(back2token)\n",
        "#     if num == 2:\n",
        "#         break\n",
        "    \n",
        "# for num, item in enumerate(train_iter):\n",
        "#     data = torch.tensor(vocab(tokenizer(item)), dtype=torch.long)\n",
        "#     print(data)\n",
        "#     if num == 2:\n",
        "#         break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load and batch data\n",
        "NOTE: batch data can enable parallel processing. However, the data in different batches will not be trained together. The model treats each batch independently. The data in different batches will not be considered as the context information. \n",
        "\n",
        "In the case of IPS, do we need batches？？\n",
        "\n",
        "For example, if we have 10 trajectories. Each trajectory has 8000 data. Suppose for each trajectory, we define 400 points. Then each point will correspond to 8000/400 = 20 CSI. We can convert the problem of regression down to classification problem, which states as follows. Given a test data with 400 CSI, it can be considered as having passed 400/20 = 20 positions. Then the task will be to predict the next position, which corresponds to the task of predicting the next word in NLP. 400 positions meas we have 400 words. Here, 400 words probably correspond to ``bptt=35`` in this tutorial. \n",
        "\n",
        "I guess 400 words can be handled by the system without resorting to batching. However, if the the data points is way larger than that. For example, if we have 10 trajectories to cover. Then the data points will be 8000 * 10 = 80000. The positions will be 400 * 10 = 4000. !!NOTE: 突然意识到一个很不错的折中的想法，batch的个数我们就可以想象为是trajectory的个数！那么一个batch，即一个trajectory中，我们就有8000的数据点，以及定义的400个位置，每个位置相差20个CSI. 每个轨迹之间不需要通过tranformer去学习关联。当然了，这个还是需要实验去进行验证，IPS的准确度和trajectory的长度的关系，假如我们让transformer每次只学1个轨迹，或者是一次学2个，3个等等时，准确度是否会增高，又或者是假如增高的话，训练难度是不是也是会异常增大！\n",
        "\n",
        "We may consider using batches to help us with the long sequences. It also makes perfect sense to batch the long sequence. Imagine that maybe the CSI of some of the positions in the trajectory CSI is affected by multipath. But it is mostly likely the neighboring CSI can have a good channel condition. Then the transformer may attend to those positions more. But since the CSI of a very far apart locations will be totally different, so the transformer probably will not attend to much to the CSI of a very far away location. Hence, it is reasonable to divide the longer trajectory CSI into batches. Then transformer can learn all the batches in parallel considering the connections between batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batchify(data: Tensor, bsz: int) -> Tensor: # bsz is the number of batches\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz \n",
        "    data = data[:seq_len * bsz] # removing the extra elements \n",
        "    data = data.view(bsz,seq_len).t().contiguous() # t() means transpose! The view changes from (bsz,seq_len) to (seq_len, bsz)\n",
        "\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([102499, 20])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # to delete\n",
        "# m = nn.LogSoftmax(dim=1)\n",
        "# loss = nn.NLLLoss()\n",
        "# loss_cr = nn.CrossEntropyLoss()\n",
        "\n",
        "# # input is of size N x C = 3 x 5\n",
        "# input = torch.randn(3, 5, requires_grad=True)\n",
        "# # each element in target has to have 0 <= value < C\n",
        "# target = torch.tensor([1, 0, 4])\n",
        "\n",
        "# print(input)\n",
        "# print(m(input))\n",
        "# print(torch.exp(m(input)))\n",
        "# print(loss(m(input), target))\n",
        "# print(loss_cr(input,target))\n",
        "# output = loss(m(input), target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Functions to generate input and target sequence\n",
        "\n",
        "``get_batch()`` generates a pair of input-target sequences for\n",
        "the transformer model. It subdivides the source data into chunks of\n",
        "length ``bptt``. For the language modeling task, the model needs the\n",
        "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
        "we’d get the following two Variables for ``i`` = 0:\n",
        "\n",
        "It should be noted that the chunks are along dimension 0, consistent\n",
        "with the ``S`` dimension in the Transformer model. The batch dimension\n",
        "``N`` is along dimension 1.\n",
        "\n",
        "Here in this example, the shape of the source (``train_data``) is [102499, 20]. Therefore, the ``full_seq_len`` is 102499, and the ``batch_size`` is 20.\n",
        "Here 20 is the result of function batchify(). What it does is divide the whole 1D sequence into 20 batches and each batch has 102499 tokens. The division is carried out in a sequential order. The first 102499 of the whole 1D sequence will be the first batch, then from 102500 to 204999 will be the second batch and so on and so forth. Here, the token has already been numberized!\n",
        "\n",
        "Here what get_batch() does is to further get the small chunk from ``tran_data`` of [102499, 20]. For example, the first chunk will be the data from 0 to 34，along with the batch dimension 20. The result will be of size [35,20]. The second chunk will be from 35 to 69. The result will still be of size [35, 20]. So how many chunks are there? 102499 / 35 = 2928 chunks. \n",
        "\n",
        "In summary, the chunk here has different meaning of batch although the name of the function is named as get_batch... So for, the chunk size (bptt) gives the impression that it is equivalent to the length of one sentence as used in the context of transformer for NLP. Usually, the length of the transformer could be around 1024. The reason why it is 35, I guess, is that to make it as a small system. The larger bptt, obviously the larger of the input matrix (longer the first dimension). It turns out that if I set the value of bptt 1000, then GPU will run out of memory, which indicates that bptt is, in a sense, exactly the same as the length of words.\n",
        "\n",
        "这里在通过IPS进行思考的话，假如我们已经确定了先暂时让每个轨迹的CSI作为一个batch，那么每个轨迹CSI有8000个数据点，对应400个点，每个点之间相距20个CSI的距离，那么这里的get_batch就是指的将这400个数据点在进行切割，不去一次性计算这400个点的attention，相互的关系等，而是定义了一个量35，即每次只看其中的35个，计算35的self-attention. 得出35个数的结果之后，进行取loss，然后再去判定下一个35个，直到400个全部学习完了。当然这里的35我们也有待去探究，到底是什么数能有最优解，又或者是我们不需要这里的分成小块，而是将400直接放进去，让transformer去计算所有的400个position的CSI之间的attention，（论文）我们甚至可以说可以将不同参数(35,或者是别的数，或者不要chunk)的attenion给打印出来，用图像的形势看看具体的位置之间CSI的attention关系，将这图放入到论文当中去！\n",
        "\n",
        "NOTE: 这里呢，有一个catch，那就是计算的时候不只是单纯的计算35个，而是35 x #batch个，即假如我们有10个轨迹的话，那么第一次学习的时候呢，就是先学习35 * 10，10个应该是同时进行的，相互之间没有关联。\n",
        "\n",
        "#NOTE: ##! The data and target has the same shape. The only difference is that for example, if the data is taken from index 0 to 35, then the target will be from 1 to 36 because target should be the prediction of the previous words. Since the data also has a batch dimension, e.g. 20 in this tutorial. For every batch, they all have 35 words, and the objective is the same for all the batches which is to predict the next word. \n",
        "So (I think !!!) for the first iteration, the first word of all the 20 batches are put in the model.\n",
        "Then the model should have 20 predictions for the next word. So our 20 targets are exactly the next 20 words. The loss will be calculated and then use GD to update the weights. Then For the second iteration, the output of the first word of 20 batches will be as the input to the model, and then the model will have 20 predictions for the third word. So our 20 target will be the next 20 true words. Then loss ... The iteration keeps on until 35 words are all learned! I am not so sure whether the real process takes place just as I wrote above in a manner of one word at a time. Or, more likely, the 35 words will be put in the model directly. I tend to believe the second scenario is true because that is exactly what a transformer do, only focus on the attentions!\n",
        "\n",
        "``NOTE!`` 这里的bptt绝对就是transformer论文中一个句子中的单词数的概念！！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bptt = 35 #NOTE: batch dimension N, the length of each chunk\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:  #NOTE: The source data has not \n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)  #! The actual value for i is [0 35 70 105,...]\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1) #! reshape(-1) will unfold the matrix from the higher dimension to the lower dimension\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([102499, 20])\n",
            "torch.Size([35, 20])\n",
            "torch.Size([35, 20])\n",
            "tensor([ 3849,    12,   300,  6302,  3989,  1930, 10559,   451,     4,     7,\n",
            "            2,  1511, 10115,   942,  2439,   572,     1,    47,    30,  1990,\n",
            "         3869,   315,    19,    29,   939,     2,    10,  2139,  4916, 16615,\n",
            "          235,     3,    13,     7,    24,    17, 13737,    97,  7720,     4],\n",
            "       device='cuda:0')\n",
            "tensor([ 3849,    12,   300,  6302,  3989,  1930, 10559,   451,     4,     7,\n",
            "            2,  1511, 10115,   942,  2439,   572,     1,    47,    30,  1990,\n",
            "         3869,   315,    19,    29,   939,     2,    10,  2139,  4916, 16615,\n",
            "          235,     3,    13,     7,    24,    17, 13737,    97,  7720,     4],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# ## test (to delate)\n",
        "# from einops import rearrange\n",
        "# i = 0\n",
        "# seq_len = min(bptt, len(train_data) - 1 - i) \n",
        "# data = train_data[i:i+seq_len]\n",
        "# # target = train_data[i+1:i+1+seq_len].reshape(-1)\n",
        "# target = train_data[i+1:i+1+seq_len]\n",
        "\n",
        "# print(train_data.shape)\n",
        "# print(data.shape)\n",
        "# print(target.shape)\n",
        "\n",
        "# print(data[1:3].reshape(-1))\n",
        "# print(target[:2].reshape(-1))\n",
        "# # print(target[:10].reshape(-1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initiate an instance\n",
        "--------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model hyperparameters are defined below. The vocab size is\n",
        "equal to the length of the vocab object.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "IPS：我们还得考虑假如每个点对应20个CSI的话，那么我们还需不需要将这20长度的CSI进行embedding，map到高纬度去~！ 这个需要做试验试一试才能知道！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ntokens = len(vocab)  # size of vocabulary\n",
        "emsize = 200  # embedding dimension\n",
        "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.2  # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## test (to delate)\n",
        "# # Example of target with class indices\n",
        "# loss = nn.CrossEntropyLoss()\n",
        "# input = torch.randn(3, 5, requires_grad=True) #! Here: 5 dimensional input\n",
        "# # The following code will generate class indices from 0 to 5-1\n",
        "# target = torch.empty(3, dtype=torch.long).random_(5) #NOTE .random_(5): Fills self tensor with numbers sampled from the discrete uniform distribution \n",
        "# output = loss(input, target) \n",
        "\n",
        "# print(input)\n",
        "# print(target)\n",
        "# print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the model\n",
        "-------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use `CrossEntropyLoss <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__\n",
        "with the `SGD <https://pytorch.org/docs/stable/generated/torch.optim.SGD.html>`__\n",
        "(stochastic gradient descent) optimizer. The learning rate is initially set to\n",
        "5.0 and follows a `StepLR <https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html>`__\n",
        "schedule. During training, we use `nn.utils.clip_grad_norm\\_ <https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html>`__\n",
        "to prevent gradients from exploding.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5.0, gamma=0.95) # after one epoch, the LR becomes 95% of the original LR\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = len(train_data) // bptt \n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)): #(0 35 70 ...)\n",
        "        data, targets = get_batch(train_data, i) # i = 0, 35,70, ... len(train_data) #! The size of data is 35 by 20, 20 is the batch size\n",
        "        batch_size = data.size(0)\n",
        "        if batch_size != bptt:  # only on last batch\n",
        "            src_mask = src_mask[:batch_size, :batch_size] \n",
        "        output = model(data, src_mask) #! The shape of the output is (35, 20, 28782)\n",
        "        loss = criterion(output.view(-1, ntokens), targets) #! out.view(-1,ntokens) will make the shape (35,20,28782) to (700,28782)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.6f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            batch_size = data.size(0)\n",
        "            if batch_size != bptt:\n",
        "                src_mask = src_mask[:batch_size, :batch_size]\n",
        "            output = model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loop over epochs. Save the model if the validation loss is the best\n",
        "we've seen so far. Adjust the learning rate after each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# print(torch.cuda.memory_allocated()/1024**2)\n",
        "# print(torch.cuda.memory_cached()/1024**2)\n",
        "# print(torch.cuda.memory_reserved())\n",
        "# print(torch.cuda.memory_summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2928 batches | lr 0.001000 | ms/batch 14.73 | loss  5.37 | ppl   213.85\n",
            "| epoch   1 |   400/ 2928 batches | lr 0.001000 | ms/batch 12.36 | loss  5.40 | ppl   221.25\n",
            "| epoch   1 |   600/ 2928 batches | lr 0.001000 | ms/batch 12.45 | loss  5.24 | ppl   189.53\n",
            "| epoch   1 |   800/ 2928 batches | lr 0.001000 | ms/batch 12.53 | loss  5.30 | ppl   200.49\n",
            "| epoch   1 |  1000/ 2928 batches | lr 0.001000 | ms/batch 12.49 | loss  5.25 | ppl   190.65\n",
            "| epoch   1 |  1200/ 2928 batches | lr 0.001000 | ms/batch 12.85 | loss  5.31 | ppl   202.44\n",
            "| epoch   1 |  1400/ 2928 batches | lr 0.001000 | ms/batch 12.51 | loss  5.36 | ppl   212.29\n",
            "| epoch   1 |  1600/ 2928 batches | lr 0.001000 | ms/batch 12.41 | loss  5.37 | ppl   214.67\n",
            "| epoch   1 |  1800/ 2928 batches | lr 0.001000 | ms/batch 12.65 | loss  5.28 | ppl   197.02\n",
            "| epoch   1 |  2000/ 2928 batches | lr 0.001000 | ms/batch 12.98 | loss  5.34 | ppl   207.79\n",
            "| epoch   1 |  2200/ 2928 batches | lr 0.001000 | ms/batch 12.69 | loss  5.21 | ppl   183.76\n",
            "| epoch   1 |  2400/ 2928 batches | lr 0.001000 | ms/batch 12.38 | loss  5.33 | ppl   206.13\n",
            "| epoch   1 |  2600/ 2928 batches | lr 0.001000 | ms/batch 12.38 | loss  5.32 | ppl   203.58\n",
            "| epoch   1 |  2800/ 2928 batches | lr 0.001000 | ms/batch 12.40 | loss  5.27 | ppl   193.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 38.88s | valid loss  5.51 | valid ppl   248.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2928 batches | lr 0.001000 | ms/batch 13.85 | loss  5.29 | ppl   198.80\n",
            "| epoch   2 |   400/ 2928 batches | lr 0.001000 | ms/batch 12.60 | loss  5.33 | ppl   206.45\n",
            "| epoch   2 |   600/ 2928 batches | lr 0.001000 | ms/batch 12.40 | loss  5.18 | ppl   176.96\n",
            "| epoch   2 |   800/ 2928 batches | lr 0.001000 | ms/batch 12.36 | loss  5.24 | ppl   189.06\n",
            "| epoch   2 |  1000/ 2928 batches | lr 0.001000 | ms/batch 12.60 | loss  5.19 | ppl   179.67\n",
            "| epoch   2 |  1200/ 2928 batches | lr 0.001000 | ms/batch 13.59 | loss  5.24 | ppl   189.14\n",
            "| epoch   2 |  1400/ 2928 batches | lr 0.001000 | ms/batch 12.64 | loss  5.28 | ppl   196.64\n",
            "| epoch   2 |  1600/ 2928 batches | lr 0.001000 | ms/batch 12.56 | loss  5.29 | ppl   197.87\n",
            "| epoch   2 |  1800/ 2928 batches | lr 0.001000 | ms/batch 12.44 | loss  5.23 | ppl   187.11\n",
            "| epoch   2 |  2000/ 2928 batches | lr 0.001000 | ms/batch 12.50 | loss  5.28 | ppl   197.05\n",
            "| epoch   2 |  2200/ 2928 batches | lr 0.001000 | ms/batch 12.65 | loss  5.13 | ppl   169.64\n",
            "| epoch   2 |  2400/ 2928 batches | lr 0.001000 | ms/batch 12.49 | loss  5.25 | ppl   190.32\n",
            "| epoch   2 |  2600/ 2928 batches | lr 0.001000 | ms/batch 12.69 | loss  5.26 | ppl   193.37\n",
            "| epoch   2 |  2800/ 2928 batches | lr 0.001000 | ms/batch 12.79 | loss  5.20 | ppl   182.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 39.06s | valid loss  5.51 | valid ppl   247.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2928 batches | lr 0.000950 | ms/batch 12.71 | loss  5.24 | ppl   188.58\n",
            "| epoch   3 |   400/ 2928 batches | lr 0.000950 | ms/batch 12.36 | loss  5.28 | ppl   196.67\n",
            "| epoch   3 |   600/ 2928 batches | lr 0.000950 | ms/batch 12.41 | loss  5.12 | ppl   167.42\n",
            "| epoch   3 |   800/ 2928 batches | lr 0.000950 | ms/batch 12.48 | loss  5.19 | ppl   180.37\n",
            "| epoch   3 |  1000/ 2928 batches | lr 0.000950 | ms/batch 12.31 | loss  5.14 | ppl   170.63\n",
            "| epoch   3 |  1200/ 2928 batches | lr 0.000950 | ms/batch 12.41 | loss  5.18 | ppl   177.19\n",
            "| epoch   3 |  1400/ 2928 batches | lr 0.000950 | ms/batch 12.33 | loss  5.22 | ppl   184.13\n",
            "| epoch   3 |  1600/ 2928 batches | lr 0.000950 | ms/batch 12.39 | loss  5.24 | ppl   189.15\n",
            "| epoch   3 |  1800/ 2928 batches | lr 0.000950 | ms/batch 12.44 | loss  5.20 | ppl   180.40\n",
            "| epoch   3 |  2000/ 2928 batches | lr 0.000950 | ms/batch 12.41 | loss  5.22 | ppl   185.46\n",
            "| epoch   3 |  2200/ 2928 batches | lr 0.000950 | ms/batch 12.43 | loss  5.06 | ppl   158.25\n",
            "| epoch   3 |  2400/ 2928 batches | lr 0.000950 | ms/batch 12.39 | loss  5.19 | ppl   180.34\n",
            "| epoch   3 |  2600/ 2928 batches | lr 0.000950 | ms/batch 12.48 | loss  5.22 | ppl   185.40\n",
            "| epoch   3 |  2800/ 2928 batches | lr 0.000950 | ms/batch 12.45 | loss  5.15 | ppl   172.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 38.12s | valid loss  5.52 | valid ppl   249.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 2928 batches | lr 0.000950 | ms/batch 12.56 | loss  5.20 | ppl   181.43\n",
            "| epoch   4 |   400/ 2928 batches | lr 0.000950 | ms/batch 12.58 | loss  5.25 | ppl   191.33\n",
            "| epoch   4 |   600/ 2928 batches | lr 0.000950 | ms/batch 12.49 | loss  5.09 | ppl   162.05\n",
            "| epoch   4 |   800/ 2928 batches | lr 0.000950 | ms/batch 12.46 | loss  5.17 | ppl   175.50\n",
            "| epoch   4 |  1000/ 2928 batches | lr 0.000950 | ms/batch 12.57 | loss  5.10 | ppl   163.69\n",
            "| epoch   4 |  1200/ 2928 batches | lr 0.000950 | ms/batch 12.64 | loss  5.14 | ppl   170.58\n",
            "| epoch   4 |  1400/ 2928 batches | lr 0.000950 | ms/batch 12.60 | loss  5.18 | ppl   178.09\n",
            "| epoch   4 |  1600/ 2928 batches | lr 0.000950 | ms/batch 12.88 | loss  5.23 | ppl   186.35\n",
            "| epoch   4 |  1800/ 2928 batches | lr 0.000950 | ms/batch 12.51 | loss  5.17 | ppl   175.22\n",
            "| epoch   4 |  2000/ 2928 batches | lr 0.000950 | ms/batch 12.49 | loss  5.18 | ppl   177.92\n",
            "| epoch   4 |  2200/ 2928 batches | lr 0.000950 | ms/batch 12.38 | loss  5.03 | ppl   152.43\n",
            "| epoch   4 |  2400/ 2928 batches | lr 0.000950 | ms/batch 12.44 | loss  5.17 | ppl   176.72\n",
            "| epoch   4 |  2600/ 2928 batches | lr 0.000950 | ms/batch 13.11 | loss  5.20 | ppl   181.87\n",
            "| epoch   4 |  2800/ 2928 batches | lr 0.000950 | ms/batch 12.55 | loss  5.12 | ppl   167.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 38.58s | valid loss  5.54 | valid ppl   255.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 2928 batches | lr 0.000950 | ms/batch 12.98 | loss  5.19 | ppl   178.66\n",
            "| epoch   5 |   400/ 2928 batches | lr 0.000950 | ms/batch 12.46 | loss  5.22 | ppl   184.85\n",
            "| epoch   5 |   600/ 2928 batches | lr 0.000950 | ms/batch 12.42 | loss  5.06 | ppl   157.80\n",
            "| epoch   5 |   800/ 2928 batches | lr 0.000950 | ms/batch 12.53 | loss  5.13 | ppl   169.50\n",
            "| epoch   5 |  1000/ 2928 batches | lr 0.000950 | ms/batch 13.26 | loss  5.06 | ppl   158.21\n",
            "| epoch   5 |  1200/ 2928 batches | lr 0.000950 | ms/batch 12.77 | loss  5.10 | ppl   164.05\n",
            "| epoch   5 |  1400/ 2928 batches | lr 0.000950 | ms/batch 12.87 | loss  5.16 | ppl   174.04\n",
            "| epoch   5 |  1600/ 2928 batches | lr 0.000950 | ms/batch 12.62 | loss  5.21 | ppl   182.41\n",
            "| epoch   5 |  1800/ 2928 batches | lr 0.000950 | ms/batch 12.80 | loss  5.13 | ppl   168.60\n",
            "| epoch   5 |  2000/ 2928 batches | lr 0.000950 | ms/batch 12.70 | loss  5.14 | ppl   171.55\n",
            "| epoch   5 |  2200/ 2928 batches | lr 0.000950 | ms/batch 12.74 | loss  4.99 | ppl   146.78\n",
            "| epoch   5 |  2400/ 2928 batches | lr 0.000950 | ms/batch 12.35 | loss  5.16 | ppl   174.35\n",
            "| epoch   5 |  2600/ 2928 batches | lr 0.000950 | ms/batch 12.17 | loss  5.18 | ppl   177.44\n",
            "| epoch   5 |  2800/ 2928 batches | lr 0.000950 | ms/batch 12.23 | loss  5.10 | ppl   163.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 38.62s | valid loss  5.58 | valid ppl   265.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 2928 batches | lr 0.000950 | ms/batch 12.97 | loss  5.16 | ppl   174.37\n",
            "| epoch   6 |   400/ 2928 batches | lr 0.000950 | ms/batch 12.25 | loss  5.19 | ppl   179.64\n",
            "| epoch   6 |   600/ 2928 batches | lr 0.000950 | ms/batch 12.23 | loss  5.04 | ppl   154.17\n",
            "| epoch   6 |   800/ 2928 batches | lr 0.000950 | ms/batch 12.26 | loss  5.10 | ppl   163.82\n",
            "| epoch   6 |  1000/ 2928 batches | lr 0.000950 | ms/batch 12.32 | loss  5.04 | ppl   153.97\n",
            "| epoch   6 |  1200/ 2928 batches | lr 0.000950 | ms/batch 12.22 | loss  5.09 | ppl   161.64\n",
            "| epoch   6 |  1400/ 2928 batches | lr 0.000950 | ms/batch 12.26 | loss  5.14 | ppl   170.07\n",
            "| epoch   6 |  1600/ 2928 batches | lr 0.000950 | ms/batch 12.21 | loss  5.19 | ppl   179.76\n",
            "| epoch   6 |  1800/ 2928 batches | lr 0.000950 | ms/batch 12.28 | loss  5.09 | ppl   162.90\n",
            "| epoch   6 |  2000/ 2928 batches | lr 0.000950 | ms/batch 12.21 | loss  5.11 | ppl   165.73\n",
            "| epoch   6 |  2200/ 2928 batches | lr 0.000950 | ms/batch 12.20 | loss  4.96 | ppl   143.01\n",
            "| epoch   6 |  2400/ 2928 batches | lr 0.000950 | ms/batch 12.44 | loss  5.14 | ppl   170.71\n",
            "| epoch   6 |  2600/ 2928 batches | lr 0.000950 | ms/batch 12.28 | loss  5.15 | ppl   172.33\n",
            "| epoch   6 |  2800/ 2928 batches | lr 0.000950 | ms/batch 12.31 | loss  5.09 | ppl   161.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 37.86s | valid loss  5.62 | valid ppl   274.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 2928 batches | lr 0.000950 | ms/batch 12.61 | loss  5.14 | ppl   171.38\n",
            "| epoch   7 |   400/ 2928 batches | lr 0.000950 | ms/batch 12.34 | loss  5.17 | ppl   176.49\n",
            "| epoch   7 |   600/ 2928 batches | lr 0.000950 | ms/batch 12.32 | loss  5.01 | ppl   150.02\n",
            "| epoch   7 |   800/ 2928 batches | lr 0.000950 | ms/batch 12.23 | loss  5.06 | ppl   158.35\n",
            "| epoch   7 |  1000/ 2928 batches | lr 0.000950 | ms/batch 12.17 | loss  5.02 | ppl   151.09\n",
            "| epoch   7 |  1200/ 2928 batches | lr 0.000950 | ms/batch 12.17 | loss  5.07 | ppl   159.58\n",
            "| epoch   7 |  1400/ 2928 batches | lr 0.000950 | ms/batch 12.20 | loss  5.13 | ppl   168.89\n",
            "| epoch   7 |  1600/ 2928 batches | lr 0.000950 | ms/batch 12.18 | loss  5.17 | ppl   175.11\n",
            "| epoch   7 |  1800/ 2928 batches | lr 0.000950 | ms/batch 12.23 | loss  5.07 | ppl   158.47\n",
            "| epoch   7 |  2000/ 2928 batches | lr 0.000950 | ms/batch 12.61 | loss  5.09 | ppl   162.49\n",
            "| epoch   7 |  2200/ 2928 batches | lr 0.000950 | ms/batch 12.22 | loss  4.95 | ppl   140.70\n",
            "| epoch   7 |  2400/ 2928 batches | lr 0.000950 | ms/batch 12.34 | loss  5.13 | ppl   169.04\n",
            "| epoch   7 |  2600/ 2928 batches | lr 0.000950 | ms/batch 12.19 | loss  5.14 | ppl   171.54\n",
            "| epoch   7 |  2800/ 2928 batches | lr 0.000950 | ms/batch 12.70 | loss  5.08 | ppl   160.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 37.67s | valid loss  5.62 | valid ppl   276.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 2928 batches | lr 0.000902 | ms/batch 12.43 | loss  5.13 | ppl   169.28\n",
            "| epoch   8 |   400/ 2928 batches | lr 0.000902 | ms/batch 12.22 | loss  5.17 | ppl   175.32\n",
            "| epoch   8 |   600/ 2928 batches | lr 0.000902 | ms/batch 12.21 | loss  4.98 | ppl   145.35\n",
            "| epoch   8 |   800/ 2928 batches | lr 0.000902 | ms/batch 12.27 | loss  5.05 | ppl   155.56\n",
            "| epoch   8 |  1000/ 2928 batches | lr 0.000902 | ms/batch 12.21 | loss  5.00 | ppl   148.31\n",
            "| epoch   8 |  1200/ 2928 batches | lr 0.000902 | ms/batch 12.31 | loss  5.06 | ppl   157.57\n",
            "| epoch   8 |  1400/ 2928 batches | lr 0.000902 | ms/batch 12.36 | loss  5.12 | ppl   166.89\n",
            "| epoch   8 |  1600/ 2928 batches | lr 0.000902 | ms/batch 12.37 | loss  5.13 | ppl   168.84\n",
            "| epoch   8 |  1800/ 2928 batches | lr 0.000902 | ms/batch 12.37 | loss  5.04 | ppl   154.72\n",
            "| epoch   8 |  2000/ 2928 batches | lr 0.000902 | ms/batch 12.20 | loss  5.07 | ppl   158.73\n",
            "| epoch   8 |  2200/ 2928 batches | lr 0.000902 | ms/batch 12.19 | loss  4.93 | ppl   137.82\n",
            "| epoch   8 |  2400/ 2928 batches | lr 0.000902 | ms/batch 12.26 | loss  5.12 | ppl   167.17\n",
            "| epoch   8 |  2600/ 2928 batches | lr 0.000902 | ms/batch 12.28 | loss  5.14 | ppl   171.32\n",
            "| epoch   8 |  2800/ 2928 batches | lr 0.000902 | ms/batch 12.93 | loss  5.06 | ppl   158.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 37.63s | valid loss  5.63 | valid ppl   278.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 2928 batches | lr 0.000902 | ms/batch 12.03 | loss  5.13 | ppl   168.82\n",
            "| epoch   9 |   400/ 2928 batches | lr 0.000902 | ms/batch 12.00 | loss  5.16 | ppl   173.67\n",
            "| epoch   9 |   600/ 2928 batches | lr 0.000902 | ms/batch 13.15 | loss  4.97 | ppl   144.55\n",
            "| epoch   9 |   800/ 2928 batches | lr 0.000902 | ms/batch 12.44 | loss  5.06 | ppl   156.81\n",
            "| epoch   9 |  1000/ 2928 batches | lr 0.000902 | ms/batch 11.86 | loss  5.00 | ppl   148.55\n",
            "| epoch   9 |  1200/ 2928 batches | lr 0.000902 | ms/batch 12.27 | loss  5.08 | ppl   160.22\n",
            "| epoch   9 |  1400/ 2928 batches | lr 0.000902 | ms/batch 12.30 | loss  5.13 | ppl   169.27\n",
            "| epoch   9 |  1600/ 2928 batches | lr 0.000902 | ms/batch 11.80 | loss  5.13 | ppl   168.47\n",
            "| epoch   9 |  1800/ 2928 batches | lr 0.000902 | ms/batch 12.05 | loss  5.03 | ppl   152.92\n",
            "| epoch   9 |  2000/ 2928 batches | lr 0.000902 | ms/batch 11.88 | loss  5.07 | ppl   159.94\n",
            "| epoch   9 |  2200/ 2928 batches | lr 0.000902 | ms/batch 12.25 | loss  4.96 | ppl   142.16\n",
            "| epoch   9 |  2400/ 2928 batches | lr 0.000902 | ms/batch 13.14 | loss  5.16 | ppl   173.82\n",
            "| epoch   9 |  2600/ 2928 batches | lr 0.000902 | ms/batch 12.17 | loss  5.16 | ppl   174.22\n",
            "| epoch   9 |  2800/ 2928 batches | lr 0.000902 | ms/batch 12.16 | loss  5.08 | ppl   160.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 37.59s | valid loss  5.64 | valid ppl   281.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 2928 batches | lr 0.000902 | ms/batch 12.15 | loss  5.13 | ppl   169.67\n",
            "| epoch  10 |   400/ 2928 batches | lr 0.000902 | ms/batch 11.82 | loss  5.17 | ppl   176.49\n",
            "| epoch  10 |   600/ 2928 batches | lr 0.000902 | ms/batch 12.19 | loss  4.99 | ppl   147.56\n",
            "| epoch  10 |   800/ 2928 batches | lr 0.000902 | ms/batch 11.79 | loss  5.08 | ppl   161.02\n",
            "| epoch  10 |  1000/ 2928 batches | lr 0.000902 | ms/batch 11.72 | loss  5.04 | ppl   154.49\n",
            "| epoch  10 |  1200/ 2928 batches | lr 0.000902 | ms/batch 11.83 | loss  5.11 | ppl   166.19\n",
            "| epoch  10 |  1400/ 2928 batches | lr 0.000902 | ms/batch 12.17 | loss  5.15 | ppl   172.10\n",
            "| epoch  10 |  1600/ 2928 batches | lr 0.000902 | ms/batch 11.84 | loss  5.12 | ppl   167.96\n",
            "| epoch  10 |  1800/ 2928 batches | lr 0.000902 | ms/batch 11.83 | loss  5.05 | ppl   155.30\n",
            "| epoch  10 |  2000/ 2928 batches | lr 0.000902 | ms/batch 12.17 | loss  5.12 | ppl   167.83\n",
            "| epoch  10 |  2200/ 2928 batches | lr 0.000902 | ms/batch 12.08 | loss  5.01 | ppl   149.70\n",
            "| epoch  10 |  2400/ 2928 batches | lr 0.000902 | ms/batch 11.79 | loss  5.18 | ppl   178.37\n",
            "| epoch  10 |  2600/ 2928 batches | lr 0.000902 | ms/batch 12.36 | loss  5.17 | ppl   176.53\n",
            "| epoch  10 |  2800/ 2928 batches | lr 0.000902 | ms/batch 12.11 | loss  5.09 | ppl   162.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 36.69s | valid loss  5.65 | valid ppl   284.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/ 2928 batches | lr 0.000902 | ms/batch 11.84 | loss  5.16 | ppl   174.58\n",
            "| epoch  11 |   400/ 2928 batches | lr 0.000902 | ms/batch 12.40 | loss  5.20 | ppl   181.95\n",
            "| epoch  11 |   600/ 2928 batches | lr 0.000902 | ms/batch 12.01 | loss  5.04 | ppl   154.03\n",
            "| epoch  11 |   800/ 2928 batches | lr 0.000902 | ms/batch 11.82 | loss  5.14 | ppl   170.40\n",
            "| epoch  11 |  1000/ 2928 batches | lr 0.000902 | ms/batch 11.80 | loss  5.08 | ppl   160.91\n",
            "| epoch  11 |  1200/ 2928 batches | lr 0.000902 | ms/batch 11.93 | loss  5.12 | ppl   167.69\n",
            "| epoch  11 |  1400/ 2928 batches | lr 0.000902 | ms/batch 11.95 | loss  5.15 | ppl   172.77\n",
            "| epoch  11 |  1600/ 2928 batches | lr 0.000902 | ms/batch 11.90 | loss  5.14 | ppl   170.66\n",
            "| epoch  11 |  1800/ 2928 batches | lr 0.000902 | ms/batch 11.69 | loss  5.09 | ppl   162.98\n",
            "| epoch  11 |  2000/ 2928 batches | lr 0.000902 | ms/batch 11.98 | loss  5.18 | ppl   177.87\n",
            "| epoch  11 |  2200/ 2928 batches | lr 0.000902 | ms/batch 11.89 | loss  5.03 | ppl   153.17\n",
            "| epoch  11 |  2400/ 2928 batches | lr 0.000902 | ms/batch 11.98 | loss  5.19 | ppl   180.28\n",
            "| epoch  11 |  2600/ 2928 batches | lr 0.000902 | ms/batch 11.79 | loss  5.17 | ppl   176.10\n",
            "| epoch  11 |  2800/ 2928 batches | lr 0.000902 | ms/batch 11.86 | loss  5.10 | ppl   163.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 36.51s | valid loss  5.66 | valid ppl   287.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/ 2928 batches | lr 0.000902 | ms/batch 12.67 | loss  5.20 | ppl   180.50\n",
            "| epoch  12 |   400/ 2928 batches | lr 0.000902 | ms/batch 12.90 | loss  5.24 | ppl   188.07\n",
            "| epoch  12 |   600/ 2928 batches | lr 0.000902 | ms/batch 12.29 | loss  5.07 | ppl   159.23\n",
            "| epoch  12 |   800/ 2928 batches | lr 0.000902 | ms/batch 14.52 | loss  5.17 | ppl   175.12\n",
            "| epoch  12 |  1000/ 2928 batches | lr 0.000902 | ms/batch 13.62 | loss  5.09 | ppl   162.47\n",
            "| epoch  12 |  1200/ 2928 batches | lr 0.000902 | ms/batch 12.04 | loss  5.12 | ppl   168.13\n",
            "| epoch  12 |  1400/ 2928 batches | lr 0.000902 | ms/batch 12.68 | loss  5.15 | ppl   173.24\n",
            "| epoch  12 |  1600/ 2928 batches | lr 0.000902 | ms/batch 12.12 | loss  5.19 | ppl   179.59\n",
            "| epoch  12 |  1800/ 2928 batches | lr 0.000902 | ms/batch 13.38 | loss  5.15 | ppl   172.76\n",
            "| epoch  12 |  2000/ 2928 batches | lr 0.000902 | ms/batch 14.07 | loss  5.21 | ppl   183.34\n",
            "| epoch  12 |  2200/ 2928 batches | lr 0.000902 | ms/batch 12.62 | loss  5.05 | ppl   156.40\n",
            "| epoch  12 |  2400/ 2928 batches | lr 0.000902 | ms/batch 12.33 | loss  5.20 | ppl   180.88\n",
            "| epoch  12 |  2600/ 2928 batches | lr 0.000902 | ms/batch 12.21 | loss  5.18 | ppl   177.15\n",
            "| epoch  12 |  2800/ 2928 batches | lr 0.000902 | ms/batch 11.85 | loss  5.12 | ppl   167.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 39.00s | valid loss  5.66 | valid ppl   287.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/ 2928 batches | lr 0.000857 | ms/batch 12.33 | loss  5.24 | ppl   188.53\n",
            "| epoch  13 |   400/ 2928 batches | lr 0.000857 | ms/batch 11.84 | loss  5.26 | ppl   192.57\n",
            "| epoch  13 |   600/ 2928 batches | lr 0.000857 | ms/batch 11.78 | loss  5.09 | ppl   162.09\n",
            "| epoch  13 |   800/ 2928 batches | lr 0.000857 | ms/batch 11.82 | loss  5.16 | ppl   175.03\n",
            "| epoch  13 |  1000/ 2928 batches | lr 0.000857 | ms/batch 11.89 | loss  5.09 | ppl   161.84\n",
            "| epoch  13 |  1200/ 2928 batches | lr 0.000857 | ms/batch 11.82 | loss  5.12 | ppl   166.66\n",
            "| epoch  13 |  1400/ 2928 batches | lr 0.000857 | ms/batch 12.16 | loss  5.17 | ppl   175.30\n",
            "| epoch  13 |  1600/ 2928 batches | lr 0.000857 | ms/batch 11.76 | loss  5.23 | ppl   187.51\n",
            "| epoch  13 |  1800/ 2928 batches | lr 0.000857 | ms/batch 12.36 | loss  5.17 | ppl   176.06\n",
            "| epoch  13 |  2000/ 2928 batches | lr 0.000857 | ms/batch 12.64 | loss  5.20 | ppl   182.14\n",
            "| epoch  13 |  2200/ 2928 batches | lr 0.000857 | ms/batch 12.04 | loss  5.05 | ppl   156.15\n",
            "| epoch  13 |  2400/ 2928 batches | lr 0.000857 | ms/batch 12.04 | loss  5.19 | ppl   178.72\n",
            "| epoch  13 |  2600/ 2928 batches | lr 0.000857 | ms/batch 11.96 | loss  5.16 | ppl   173.88\n",
            "| epoch  13 |  2800/ 2928 batches | lr 0.000857 | ms/batch 11.98 | loss  5.12 | ppl   168.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 36.87s | valid loss  5.66 | valid ppl   286.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/ 2928 batches | lr 0.000857 | ms/batch 13.00 | loss  5.25 | ppl   190.23\n",
            "| epoch  14 |   400/ 2928 batches | lr 0.000857 | ms/batch 11.83 | loss  5.28 | ppl   195.49\n",
            "| epoch  14 |   600/ 2928 batches | lr 0.000857 | ms/batch 11.74 | loss  5.10 | ppl   163.32\n",
            "| epoch  14 |   800/ 2928 batches | lr 0.000857 | ms/batch 12.30 | loss  5.18 | ppl   177.16\n",
            "| epoch  14 |  1000/ 2928 batches | lr 0.000857 | ms/batch 12.39 | loss  5.09 | ppl   161.72\n",
            "| epoch  14 |  1200/ 2928 batches | lr 0.000857 | ms/batch 12.06 | loss  5.13 | ppl   169.33\n",
            "| epoch  14 |  1400/ 2928 batches | lr 0.000857 | ms/batch 12.22 | loss  5.22 | ppl   185.46\n",
            "| epoch  14 |  1600/ 2928 batches | lr 0.000857 | ms/batch 11.84 | loss  5.27 | ppl   195.07\n",
            "| epoch  14 |  1800/ 2928 batches | lr 0.000857 | ms/batch 11.89 | loss  5.19 | ppl   178.69\n",
            "| epoch  14 |  2000/ 2928 batches | lr 0.000857 | ms/batch 12.37 | loss  5.22 | ppl   185.58\n",
            "| epoch  14 |  2200/ 2928 batches | lr 0.000857 | ms/batch 11.77 | loss  5.05 | ppl   156.72\n",
            "| epoch  14 |  2400/ 2928 batches | lr 0.000857 | ms/batch 11.79 | loss  5.19 | ppl   180.00\n",
            "| epoch  14 |  2600/ 2928 batches | lr 0.000857 | ms/batch 11.80 | loss  5.18 | ppl   178.33\n",
            "| epoch  14 |  2800/ 2928 batches | lr 0.000857 | ms/batch 11.94 | loss  5.17 | ppl   175.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 36.98s | valid loss  5.66 | valid ppl   285.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/ 2928 batches | lr 0.000857 | ms/batch 12.83 | loss  5.26 | ppl   192.64\n",
            "| epoch  15 |   400/ 2928 batches | lr 0.000857 | ms/batch 11.95 | loss  5.29 | ppl   199.07\n",
            "| epoch  15 |   600/ 2928 batches | lr 0.000857 | ms/batch 11.78 | loss  5.11 | ppl   166.05\n",
            "| epoch  15 |   800/ 2928 batches | lr 0.000857 | ms/batch 12.05 | loss  5.17 | ppl   175.97\n",
            "| epoch  15 |  1000/ 2928 batches | lr 0.000857 | ms/batch 12.25 | loss  5.09 | ppl   162.19\n",
            "| epoch  15 |  1200/ 2928 batches | lr 0.000857 | ms/batch 12.12 | loss  5.16 | ppl   173.49\n",
            "| epoch  15 |  1400/ 2928 batches | lr 0.000857 | ms/batch 11.93 | loss  5.26 | ppl   192.98\n",
            "| epoch  15 |  1600/ 2928 batches | lr 0.000857 | ms/batch 12.02 | loss  5.30 | ppl   199.80\n",
            "| epoch  15 |  1800/ 2928 batches | lr 0.000857 | ms/batch 11.83 | loss  5.20 | ppl   181.68\n",
            "| epoch  15 |  2000/ 2928 batches | lr 0.000857 | ms/batch 12.00 | loss  5.22 | ppl   185.79\n",
            "| epoch  15 |  2200/ 2928 batches | lr 0.000857 | ms/batch 13.22 | loss  5.05 | ppl   156.61\n",
            "| epoch  15 |  2400/ 2928 batches | lr 0.000857 | ms/batch 11.93 | loss  5.20 | ppl   180.38\n",
            "| epoch  15 |  2600/ 2928 batches | lr 0.000857 | ms/batch 11.78 | loss  5.23 | ppl   187.15\n",
            "| epoch  15 |  2800/ 2928 batches | lr 0.000857 | ms/batch 11.98 | loss  5.20 | ppl   180.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 37.01s | valid loss  5.65 | valid ppl   285.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/ 2928 batches | lr 0.000857 | ms/batch 12.03 | loss  5.29 | ppl   197.52\n",
            "| epoch  16 |   400/ 2928 batches | lr 0.000857 | ms/batch 12.15 | loss  5.30 | ppl   200.03\n",
            "| epoch  16 |   600/ 2928 batches | lr 0.000857 | ms/batch 12.08 | loss  5.11 | ppl   165.42\n",
            "| epoch  16 |   800/ 2928 batches | lr 0.000857 | ms/batch 11.89 | loss  5.16 | ppl   174.63\n",
            "| epoch  16 |  1000/ 2928 batches | lr 0.000857 | ms/batch 11.90 | loss  5.11 | ppl   164.95\n",
            "| epoch  16 |  1200/ 2928 batches | lr 0.000857 | ms/batch 11.83 | loss  5.20 | ppl   181.10\n",
            "| epoch  16 |  1400/ 2928 batches | lr 0.000857 | ms/batch 11.76 | loss  5.29 | ppl   198.35\n",
            "| epoch  16 |  1600/ 2928 batches | lr 0.000857 | ms/batch 11.80 | loss  5.32 | ppl   203.78\n",
            "| epoch  16 |  1800/ 2928 batches | lr 0.000857 | ms/batch 12.11 | loss  5.21 | ppl   182.38\n",
            "| epoch  16 |  2000/ 2928 batches | lr 0.000857 | ms/batch 12.04 | loss  5.22 | ppl   184.96\n",
            "| epoch  16 |  2200/ 2928 batches | lr 0.000857 | ms/batch 12.09 | loss  5.05 | ppl   155.31\n",
            "| epoch  16 |  2400/ 2928 batches | lr 0.000857 | ms/batch 11.83 | loss  5.22 | ppl   185.81\n",
            "| epoch  16 |  2600/ 2928 batches | lr 0.000857 | ms/batch 12.91 | loss  5.26 | ppl   191.96\n",
            "| epoch  16 |  2800/ 2928 batches | lr 0.000857 | ms/batch 11.84 | loss  5.22 | ppl   184.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 36.81s | valid loss  5.67 | valid ppl   290.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/ 2928 batches | lr 0.000857 | ms/batch 12.31 | loss  5.29 | ppl   198.98\n",
            "| epoch  17 |   400/ 2928 batches | lr 0.000857 | ms/batch 11.95 | loss  5.30 | ppl   199.65\n",
            "| epoch  17 |   600/ 2928 batches | lr 0.000857 | ms/batch 11.88 | loss  5.10 | ppl   163.91\n",
            "| epoch  17 |   800/ 2928 batches | lr 0.000857 | ms/batch 11.80 | loss  5.17 | ppl   175.28\n",
            "| epoch  17 |  1000/ 2928 batches | lr 0.000857 | ms/batch 12.45 | loss  5.14 | ppl   170.28\n",
            "| epoch  17 |  1200/ 2928 batches | lr 0.000857 | ms/batch 11.74 | loss  5.23 | ppl   186.71\n",
            "| epoch  17 |  1400/ 2928 batches | lr 0.000857 | ms/batch 12.43 | loss  5.31 | ppl   202.43\n",
            "| epoch  17 |  1600/ 2928 batches | lr 0.000857 | ms/batch 11.96 | loss  5.33 | ppl   206.91\n",
            "| epoch  17 |  1800/ 2928 batches | lr 0.000857 | ms/batch 12.11 | loss  5.21 | ppl   182.20\n",
            "| epoch  17 |  2000/ 2928 batches | lr 0.000857 | ms/batch 11.82 | loss  5.21 | ppl   183.85\n",
            "| epoch  17 |  2200/ 2928 batches | lr 0.000857 | ms/batch 12.00 | loss  5.06 | ppl   157.72\n",
            "| epoch  17 |  2400/ 2928 batches | lr 0.000857 | ms/batch 11.74 | loss  5.27 | ppl   195.08\n",
            "| epoch  17 |  2600/ 2928 batches | lr 0.000857 | ms/batch 11.78 | loss  5.28 | ppl   195.96\n",
            "| epoch  17 |  2800/ 2928 batches | lr 0.000857 | ms/batch 11.91 | loss  5.23 | ppl   186.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 36.65s | valid loss  5.66 | valid ppl   287.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/ 2928 batches | lr 0.000815 | ms/batch 12.12 | loss  5.29 | ppl   198.79\n",
            "| epoch  18 |   400/ 2928 batches | lr 0.000815 | ms/batch 12.46 | loss  5.29 | ppl   197.65\n",
            "| epoch  18 |   600/ 2928 batches | lr 0.000815 | ms/batch 12.07 | loss  5.09 | ppl   161.86\n",
            "| epoch  18 |   800/ 2928 batches | lr 0.000815 | ms/batch 12.11 | loss  5.18 | ppl   177.81\n",
            "| epoch  18 |  1000/ 2928 batches | lr 0.000815 | ms/batch 12.10 | loss  5.16 | ppl   173.97\n",
            "| epoch  18 |  1200/ 2928 batches | lr 0.000815 | ms/batch 11.84 | loss  5.24 | ppl   187.86\n",
            "| epoch  18 |  1400/ 2928 batches | lr 0.000815 | ms/batch 12.30 | loss  5.31 | ppl   201.36\n",
            "| epoch  18 |  1600/ 2928 batches | lr 0.000815 | ms/batch 12.52 | loss  5.31 | ppl   202.51\n",
            "| epoch  18 |  1800/ 2928 batches | lr 0.000815 | ms/batch 11.74 | loss  5.19 | ppl   179.02\n",
            "| epoch  18 |  2000/ 2928 batches | lr 0.000815 | ms/batch 12.08 | loss  5.19 | ppl   179.36\n",
            "| epoch  18 |  2200/ 2928 batches | lr 0.000815 | ms/batch 12.00 | loss  5.07 | ppl   159.58\n",
            "| epoch  18 |  2400/ 2928 batches | lr 0.000815 | ms/batch 11.83 | loss  5.27 | ppl   194.51\n",
            "| epoch  18 |  2600/ 2928 batches | lr 0.000815 | ms/batch 11.94 | loss  5.28 | ppl   196.13\n",
            "| epoch  18 |  2800/ 2928 batches | lr 0.000815 | ms/batch 12.55 | loss  5.22 | ppl   184.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 37.32s | valid loss  5.66 | valid ppl   287.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/ 2928 batches | lr 0.000815 | ms/batch 12.35 | loss  5.28 | ppl   195.68\n",
            "| epoch  19 |   400/ 2928 batches | lr 0.000815 | ms/batch 12.06 | loss  5.26 | ppl   193.11\n",
            "| epoch  19 |   600/ 2928 batches | lr 0.000815 | ms/batch 12.09 | loss  5.09 | ppl   162.80\n",
            "| epoch  19 |   800/ 2928 batches | lr 0.000815 | ms/batch 11.96 | loss  5.20 | ppl   180.96\n",
            "| epoch  19 |  1000/ 2928 batches | lr 0.000815 | ms/batch 11.75 | loss  5.18 | ppl   176.83\n",
            "| epoch  19 |  1200/ 2928 batches | lr 0.000815 | ms/batch 11.83 | loss  5.24 | ppl   189.27\n",
            "| epoch  19 |  1400/ 2928 batches | lr 0.000815 | ms/batch 11.80 | loss  5.30 | ppl   200.34\n",
            "| epoch  19 |  1600/ 2928 batches | lr 0.000815 | ms/batch 11.99 | loss  5.30 | ppl   200.31\n",
            "| epoch  19 |  1800/ 2928 batches | lr 0.000815 | ms/batch 12.05 | loss  5.17 | ppl   176.06\n",
            "| epoch  19 |  2000/ 2928 batches | lr 0.000815 | ms/batch 12.08 | loss  5.20 | ppl   181.42\n",
            "| epoch  19 |  2200/ 2928 batches | lr 0.000815 | ms/batch 11.77 | loss  5.10 | ppl   164.03\n",
            "| epoch  19 |  2400/ 2928 batches | lr 0.000815 | ms/batch 11.82 | loss  5.29 | ppl   198.79\n",
            "| epoch  19 |  2600/ 2928 batches | lr 0.000815 | ms/batch 12.12 | loss  5.29 | ppl   197.93\n",
            "| epoch  19 |  2800/ 2928 batches | lr 0.000815 | ms/batch 11.94 | loss  5.22 | ppl   185.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 36.66s | valid loss  5.65 | valid ppl   284.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/ 2928 batches | lr 0.000815 | ms/batch 12.07 | loss  5.26 | ppl   191.82\n",
            "| epoch  20 |   400/ 2928 batches | lr 0.000815 | ms/batch 11.91 | loss  5.27 | ppl   193.61\n",
            "| epoch  20 |   600/ 2928 batches | lr 0.000815 | ms/batch 12.12 | loss  5.11 | ppl   165.82\n",
            "| epoch  20 |   800/ 2928 batches | lr 0.000815 | ms/batch 12.17 | loss  5.22 | ppl   184.86\n",
            "| epoch  20 |  1000/ 2928 batches | lr 0.000815 | ms/batch 12.12 | loss  5.18 | ppl   177.88\n",
            "| epoch  20 |  1200/ 2928 batches | lr 0.000815 | ms/batch 11.87 | loss  5.24 | ppl   188.80\n",
            "| epoch  20 |  1400/ 2928 batches | lr 0.000815 | ms/batch 12.02 | loss  5.29 | ppl   198.25\n",
            "| epoch  20 |  1600/ 2928 batches | lr 0.000815 | ms/batch 12.22 | loss  5.28 | ppl   196.57\n",
            "| epoch  20 |  1800/ 2928 batches | lr 0.000815 | ms/batch 12.45 | loss  5.17 | ppl   176.03\n",
            "| epoch  20 |  2000/ 2928 batches | lr 0.000815 | ms/batch 12.35 | loss  5.23 | ppl   186.07\n",
            "| epoch  20 |  2200/ 2928 batches | lr 0.000815 | ms/batch 11.83 | loss  5.11 | ppl   166.47\n",
            "| epoch  20 |  2400/ 2928 batches | lr 0.000815 | ms/batch 11.76 | loss  5.31 | ppl   201.80\n",
            "| epoch  20 |  2600/ 2928 batches | lr 0.000815 | ms/batch 11.83 | loss  5.29 | ppl   199.00\n",
            "| epoch  20 |  2800/ 2928 batches | lr 0.000815 | ms/batch 12.54 | loss  5.21 | ppl   182.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 37.03s | valid loss  5.67 | valid ppl   288.64\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 20\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the best model on the test dataset\n",
        "-------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.42 | test ppl   226.74\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # to delete\n",
        "# best_model.eval()  # turn on evaluation mode\n",
        "# total_loss = 0.\n",
        "# src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "# with torch.no_grad():\n",
        "#     for i in range(0, val_data.size(0) - 1, bptt):\n",
        "#         data, targets = get_batch(val_data, i)\n",
        "#         batch_size = data.size(0)\n",
        "#         if batch_size != bptt:\n",
        "#             src_mask = src_mask[:batch_size, :batch_size]\n",
        "#         output = model(data, src_mask)\n",
        "#         print(output.shape)\n",
        "#         output_flat = output.view(-1, ntokens)\n",
        "#         print(output_flat.shape)\n",
        "#         print(output_flat.argmax(1)[:10])\n",
        "#         print(f'target shape is {targets.shape}')\n",
        "#         print(targets[:10])\n",
        "#         break\n",
        "#         # total_loss += batch_size * criterion(output_flat, targets).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "# val_data = batchify(val_data, eval_batch_size)\n",
        "# test_data = batchify(test_data, eval_batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to delete from the Internet for future reference\n",
        "# def evaluate(dataloader):\n",
        "#     model.eval()\n",
        "#     total_acc, total_count = 0, 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "#             predicted_label = model(text, offsets)\n",
        "#             loss = criterion(predicted_label, label)\n",
        "#             total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "#             total_count += label.size(0)\n",
        "#     return total_acc/total_count"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
