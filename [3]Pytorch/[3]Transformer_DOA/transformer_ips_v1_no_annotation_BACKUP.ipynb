{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 517,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the model\n",
        "----------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 585,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, \n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout) #NOTE d_model is the embedding size\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        # self.encoder = nn.Embedding(ntoken, d_model) # NOTE: Do not need embedding for IPS, the data itself has 384 dimensional data\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "        self.doa_embedding = DOAEncoding(d_model)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        # self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_doa:Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size] #! Make the src to be a tuple, (csi, doa)\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        # src = self.encoder(src) * math.sqrt(self.d_model) #! the input is 35(just consider it to be # of batches) by 20, the output is 35 by 20 by 200. The embedding turns the indices into vectors of size 200.\n",
        "        \n",
        "        # note: with DOA         \n",
        "        src_doa = self.doa_embedding(src_doa) #!HACK: NOTE: DOA embedding should be in from of the pos encoding !!\n",
        "        # print('src and src_doa shape', src.shape, src_doa.shape)\n",
        "        # src = src + src_doa\n",
        "        src = src_doa\n",
        "\n",
        "        src = self.pos_encoder(src)\n",
        "        \n",
        "        output = self.transformer_encoder(src, src_mask) #! the output is the output of the final fully-connected layer of 200 dimension. The dimension here is still the same as 35 by 20 by 200\n",
        "        output = self.decoder(output) #! The linear layer in the decoder maps the input from 35 by 20 by 200 to 35 by 20 by ntoken (好像是两万多)\n",
        "        return output\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 519,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000): # max_len means the maximum time steps or word length\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout) # do not understand why you need dropout here\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)) #1/10000^(2i/dim_model)) #! exp(ln(x))=x, therefore, exp(ln(1/10000^(2i/dim_model)))) = exp(2i/dim_model)*(-ln(10000))\n",
        "        pe = torch.zeros(max_len, 1, d_model) #NOTE: Row always means the \n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term) #PE(pos, 2i) = sin(pos/10000^(2i/dim_model))\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term) #PE(pos, 2i) = cos(pos/10000^(2i/dim_model))\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 544,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DOAEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1): # max_len means the maximum time steps or word length\n",
        "        super().__init__()\n",
        "        # self.dropout = nn.Dropout(p=dropout) # do not understand why you need dropout here\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=512, # zzf: this value depends on the unique value of the DoA\n",
        "                               embedding_dim=d_model)\n",
        "                            #    padding_idx=1)\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        return self.embedding_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load and batch data\n",
        "-------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 521,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import scipy.io as sio\n",
        "from scipy.io import loadmat, savemat\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange,reduce,repeat\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### load training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 545,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([24, 8, 4, 3871])"
            ]
          },
          "execution_count": 545,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataPath = './matlab_input_data/2/features_1d_30snr' #\n",
        "# labelPath = 'label_10cm'\n",
        "input_data_train = sio.loadmat(dataPath)\n",
        "input_data_train = torch.from_numpy(input_data_train['features'])\n",
        "# labels = sio.loadmat(labelPath)\n",
        "# labels = torch.from_numpy(labels['label'] )\n",
        "input_data_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Shuffle the data set since all the samples are arranged in the order where SNR being -10 to 20 sequentially. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input_data_reshape = rearrange(input_data_train, 'a b c d e-> d a b c e')\n",
        "# print(input_data_reshape.shape)\n",
        "# rng = np.random.default_rng()\n",
        "# input_data_shuffle = rng.permuted(input_data_reshape,axis=0) # axis = 0, shuffle the lowest dimension. \n",
        "# print('shuffled data size is', input_data_shuffle.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### preprocessing training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 546,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_train_data_reshape torch.Size([3871, 4, 8, 24])\n",
            "training data size torch.Size([79, 49, 4, 8, 24])\n",
            "training data size torch.Size([79, 49, 768])\n"
          ]
        }
      ],
      "source": [
        "n_trajectories = 49\n",
        "n_pts_trajectory = 79\n",
        "# Change the original data dimension [3871, 4, 4, 24] to [79 x 49 x 4 x 4 x 24] \n",
        "## NOTE: The following code is for training dataset of multiple SNR case\n",
        "# input_data_reshape = rearrange(torch.from_numpy(input_data_shuffle), 'a b c d e-> e a d c b')\n",
        "## NOTE: The following code is for training dataset of single SNR case\n",
        "input_train_data_reshape = rearrange((input_data_train), 'a b c d-> d c b a')\n",
        "print('input_train_data_reshape',input_train_data_reshape.shape)\n",
        "train_data_trajectory = input_train_data_reshape.chunk(n_pts_trajectory) #! divide the data into chunks\n",
        "train_data = torch.stack([item for item in train_data_trajectory]) # [79 x 49 x 4 x 4 x 24]\n",
        "print('training data size',train_data.shape) #note the third dimension is # of SNR from -10 to 30\n",
        "\n",
        "# print(f'the number of traing and test dataset is {len(train_data)} and {len(test_data)}')\n",
        "# print(f'the size of the traning data is {train_data.shape}')\n",
        "# change the size of the from [79 x 49 x 4 x 4 x 24] to [79 x 49 x 384]\n",
        "train_data = rearrange(train_data, 'a b c d e-> a b (c d e)').to(device)\n",
        "print('training data size',train_data.shape) #note the third dimension is # of SNR from -10 to 30\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### load DOA data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 577,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "doa data size torch.Size([79, 49])\n"
          ]
        }
      ],
      "source": [
        "dataPath = './matlab_input_data/2/doa_input_1d_30snr' #\n",
        "input_data_doa = sio.loadmat(dataPath)\n",
        "input_data_doa = torch.from_numpy(input_data_doa['doa_input'])\n",
        "input_data_doa = torch.squeeze(input_data_doa)\n",
        "input_data_doa = input_data_doa.type(torch.cuda.LongTensor) #! HACK: Embedding layer only accepts the LongTensor Type\n",
        "data_doa_chunk = input_data_doa.chunk(n_pts_trajectory) #! divide the data into chunks\n",
        "train_data_doa = torch.stack([item for item in data_doa_chunk]).to(device) # [79 x 49 x 4 x 4 x 24]\n",
        "print('doa data size',train_data_doa.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 530,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3871])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(392, device='cuda:0')"
            ]
          },
          "execution_count": 530,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(train_data_doa.shape)\n",
        "max(train_data_doa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### load test data (snr = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 431,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the input data size is torch.Size([24, 4, 4, 41, 3871])\n"
          ]
        }
      ],
      "source": [
        "dataPath = './matlab_input_data/features_minus10_to_30snr'\n",
        "# labelPath = 'labels_1d_0snr'\n",
        "input_data_test = sio.loadmat(dataPath)\n",
        "input_data_test = torch.from_numpy(input_data_test['features'])\n",
        "# labels_test = sio.loadmat(labelPath)\n",
        "# labels_test = torch.from_numpy(labels_test['label'] )\n",
        "print('the input data size is',input_data_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Preprocess test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 432,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test data size torch.Size([79, 49, 41, 4, 4, 24])\n",
            "the size of the test data is torch.Size([41, 79, 49, 384])\n"
          ]
        }
      ],
      "source": [
        "input_test_data_reshape = rearrange(input_data_test, 'a b c d e -> e d c b a')\n",
        "\n",
        "data_trajectory_test = input_test_data_reshape.chunk(n_pts_trajectory) #! divide the data into chunks\n",
        "data_stack_test = torch.stack([item for item in data_trajectory_test]) # [79 x 49 x 4 x 4 x 24]\n",
        "print('test data size',data_stack_test.shape)\n",
        "\n",
        "## NOTE: The following code is for training dataset of multiple SNR case\n",
        "test_data = rearrange(data_stack_test, 'a b c d e f-> c a b (d e f)').to(device) #NOTE: the first dimension is # of SNR (-10:30 dB)\n",
        "## NOTE: The following code is for training dataset of single SNR case\n",
        "# input_data_reshape_test = rearrange(input_data_test, 'a b c d -> d c b a')\n",
        "print(f'the size of the test data is {test_data.shape}')\n",
        "\n",
        "# # Creating labels from 0 to 3870"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generate Labels for all positions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 531,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label shape torch.Size([79, 49])\n"
          ]
        }
      ],
      "source": [
        "labels_ips = torch.arange(0,3871)\n",
        "# Change the label dimension from 1d to [79 x 49]\n",
        "labels_ips = labels_ips.reshape(49, 79).t().to(device)\n",
        "print('label shape',labels_ips.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 532,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_pts_once = 40 #NOTE: batch dimension N, the length of each batch. In IPS, it means the number of points considered at once\n",
        "def get_batch(source: Tensor, source_doa: Tensor, label: Tensor, i: int) -> Tuple[Tensor, Tensor]: \n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [num_pts, batch_size] NOTE: batch_size = # of trajectories\n",
        "        label: Tensor, shape [num_pts, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [num_pts, batch_size] and\n",
        "        target has shape [num_pts * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(n_pts_once, len(source) - 1 - i)  #! The actual value for i is [0 35 70 105,...]\n",
        "    data = source[i:i+seq_len]\n",
        "    data_doa = source_doa[i:i+seq_len]\n",
        "    target = label[i+1:i+1+seq_len].reshape(-1) #! reshape(-1) will unfold the matrix from the higher dimension to the lower dimension\n",
        "    return data,data_doa,target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initiate an instance\n",
        "--------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 586,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ntokens = 3871  # there are 3871 points!!\n",
        "emsize = 768  # 384 # the data sample dimension for each point (similar to embedding dimension here)\n",
        "d_hid = 1024  # dimension of the feedforward network model in nn.TransformerEncoder %NOTE: default is 2048\n",
        "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 4  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.1  # dropout probability % 0.2\n",
        "traj_length = 79\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the model\n",
        "-------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 592,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 15.0, gamma=0.95) # after one epoch, the LR becomes 95% of the original LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 535,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([79, 49, 384])"
            ]
          },
          "execution_count": 535,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = test_data[25]\n",
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 590,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 10\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(n_pts_once).to(device)\n",
        "    num_batches = traj_length // n_pts_once\n",
        "    \n",
        "    # for    batch, i in enumerate(range(0, train_data.size(0) - 1, n_pts_once)): #(0 35 70 ...) #NOTE: `-1` is for batching the target value\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, 1)): #(0 35 70 ...) #NOTE: `-1` is for batching the target value\n",
        "        data, data_doa, targets = get_batch(train_data,train_data_doa, labels_ips, i) # i = 0, 35,70, ... len(train_data) #! The size of data is 35 by 20, 20 is the batch size\n",
        "        batch_size = data.size(0)\n",
        "\n",
        "        if batch_size != n_pts_once:  # only on last batch\n",
        "            src_mask = src_mask[:batch_size, :batch_size] \n",
        "        output = model(data, data_doa, src_mask) #! The shape of the output is (35, 20, 28782)\n",
        "        loss = criterion(output.view(-1, ntokens), targets) #! out.view(-1,ntokens) will make the shape (35,20,28782) to (700,28782)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        \n",
        "\n",
        "        # total_loss += loss.item()\n",
        "        # if batch % log_interval == 0 and batch > 0:\n",
        "        #     lr = scheduler.get_last_lr()[0]\n",
        "        #     ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "        #     cur_loss = total_loss / log_interval\n",
        "        #     ppl = math.exp(cur_loss)\n",
        "        #     print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "        #             f'lr {lr:02.6f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "        #             f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "        #     total_loss = 0\n",
        "        #     start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor, eval_data_doa:Tensor, labels: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(n_pts_once).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, n_pts_once):\n",
        "            data, data_doa,targets = get_batch(eval_data, eval_data_doa,labels, i)\n",
        "            batch_size = data.size(0)\n",
        "            if batch_size != n_pts_once:\n",
        "                src_mask = src_mask[:batch_size, :batch_size]\n",
        "            output = model(data, data_doa,src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loop over epochs. Save the model if the validation loss is the best\n",
        "we've seen so far. Adjust the learning rate after each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 593,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time:  1.13s | valid loss  0.11 | valid ppl     1.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time:  1.14s | valid loss  0.11 | valid ppl     1.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time:  1.15s | valid loss  0.10 | valid ppl     1.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  80 | time:  1.16s | valid loss  0.11 | valid ppl     1.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 100 | time:  1.16s | valid loss  0.10 | valid ppl     1.11\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "best_val_loss = float('inf') #float('inf') 1.8\n",
        "epochs = 100\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    # for snr in range(len(train_data)):\n",
        "    #     train_train_snr = train_data[snr].to(device)\n",
        "    train(model)\n",
        "    # train_train_snr_test = test_data[30].to(device)\n",
        "    val_loss = evaluate(model, train_data,train_data_doa, labels_ips)  # val_data\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    \n",
        "    if epoch % 20 == 0:\n",
        "        print('-' * 89)\n",
        "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "        print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "        lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "        # Save the model \n",
        "        fileName = f'model_numLayer_{nlayers}_numHead_{nhead}_dropout_{dropout}_batchsize_{n_pts_once}_minLoss_{val_loss:.3f}_lr_{lr:.6f}_epoch_{epoch}.pth'\n",
        "        PATH = 'model_doa_30dBsNR_8antenna_doaDataTrainingOnly'\n",
        "        if not os.path.exists(PATH):\n",
        "            os.makedirs(PATH)\n",
        "        fullPath = os.path.join(PATH, fileName)\n",
        "        # searchWord = f'model_numLayer_{nlayers}_numHead_{nhead}_dropout_{dropout}_batchsize_{n_pts_once}'\n",
        "        # files = glob.glob(f'model/{searchWord}*.pth')\n",
        "        # for i in files:\n",
        "        #     os.remove(i)\n",
        "        torch.save(best_model.state_dict(), fullPath)\n",
        "\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fileName = f'model_numLayer_{nlayers}_numHead_{nhead}_dropout_{dropout}_batchsize_{n_pts_once}_minLoss_{val_loss:.3f}_lr_{lr:.6f}_epoch_{epoch}.pth'\n",
        "PATH = 'model'\n",
        "if not os.path.exists(PATH):\n",
        "    os.makedirs(PATH)\n",
        "fullPath = os.path.join(PATH, fileName)\n",
        "# searchWord = f'model_numLayer_{nlayers}_numHead_{nhead}_dropout_{dropout}_batchsize_{n_pts_once}'\n",
        "# files = glob.glob(f'model/{searchWord}*.pth')\n",
        "# for i in files:\n",
        "#     os.remove(i)\n",
        "torch.save(best_model.state_dict(), fullPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 419,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
              "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=384, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=384, bias=True)\n",
              "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Linear(in_features=384, out_features=3871, bias=True)\n",
              "  (doa_embedding): DOAEncoding(\n",
              "    (embedding_layer): Embedding(720, 384)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 419,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the model\n",
        "new_model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
        "# new_model.load_state_dict(torch.load('model_doa_singleSNR\\model_numLayer_2_numHead_4_dropout_0.1_batchsize_40_minLoss_0.069_lr_0.000081_epoch_114.pth'))\n",
        "new_model.load_state_dict(torch.load('model_doa_singleSNR\\model_numLayer_2_numHead_4_dropout_0.1_batchsize_40_minLoss_5.835_lr_0.000095_epoch_100.pth'))\n",
        "\n",
        "# new_model.load_state_dict(torch.load('model\\model_numLayer_2_numHead_4_dropout_0.1_batchsize_40_minLoss_0.014_lr_0.000060.pth'))\n",
        "new_model.eval() # to turn off the dropout layer .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the best model on the test dataset\n",
        "-------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# test_loss = evaluate(best_model, test_data)\n",
        "# test_ppl = math.exp(test_loss)\n",
        "# print('=' * 89)\n",
        "# print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "#       f'test ppl {test_ppl:8.2f}')\n",
        "# print('=' * 89)\n",
        "def predict1(model: nn.Module, start_point: Tensor, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(n_pts_once).to(device)\n",
        "    with torch.no_grad():\n",
        "      #   for i in range(0, eval_data.size(0) - 1, n_pts_once):\n",
        "      # data, targets = get_batch(eval_data, labels, i)\n",
        "      i = start_point\n",
        "      seq_len = min(n_pts_once, len(eval_data) - 1 - i)  #! The actual value for i is [0 n_pts_once 2*n_pts_once ...]\n",
        "      data = eval_data[i:i+seq_len]\n",
        "      # target = label[i+1:i+1+seq_len].reshape(-1) #! reshape(-1) will unfold the matrix from the higher dimension to the lower dimension            \n",
        "      batch_size = data.size(0)\n",
        "      if batch_size != n_pts_once:\n",
        "            src_mask = src_mask[:batch_size, :batch_size]\n",
        "      output = model(data, src_mask)\n",
        "      output_flat = output.view(-1, ntokens)\n",
        "            \n",
        "            # total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "    return output_flat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Estimation Error Table !\n",
        "1. First, I need to create a 49 x 79 table.\n",
        "2. 49 is along the x axis, and 79 is along the y axis.\n",
        "3. The spacing between each point is 0.1cm. \n",
        "4. When calculating the error, just need to calculate the distance between the estimated "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_error(table, trajectory_ind, start_point, predicted, spacing):\n",
        "    data_length = len(predicted)\n",
        "    label = table[trajectory_ind][start_point + 1:start_point+data_length + 1]\n",
        "    predicted_ind = predicted.cpu().numpy()\n",
        "    dis_sum = 0\n",
        "    if len(label) != len(predicted):\n",
        "        predicted_ind = predicted_ind[:-1]\n",
        "    # print((predicted_ind), (label))\n",
        "    for x, y in zip(predicted_ind, label):\n",
        "        pred = np.where(table == y) #NOTE the index of the predicted value in the table\n",
        "        grd = np.where(table == x) #NOTE the index of the true label in the table\n",
        "        dis = ((pred[0] - grd[0])**2 + (pred[1] - grd[1])**2)**0.5\n",
        "        \n",
        "        \n",
        "        real_dis = dis * spacing #NOTE: Multiply with the unit\n",
        "        dis_sum = dis_sum + real_dis\n",
        "    ave = dis_sum/data_length # calculate the averaged value\n",
        "    # print(f'the averaged value over {data_length} data points is {ave} cm')\n",
        "    return ave"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Scenario 1: \n",
        "1. the test data is the same data as the training data.\n",
        "2. the batch size is the same as the training data. (batch_sie <==> n_pts_once)\n",
        "\n",
        "`Result`: The result is 100% accurate if the same data is put into the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([49, 79, 384])\n",
            "torch.Size([79, 1, 384])\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "forward() missing 1 required positional argument: 'src_mask'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\HYPC300\\OneDrive - 한양대학교\\GitHub\\Deeplearning-Autoencoder-DOA\\[3]Pytorch\\[3]Transformer_DOA\\transformer_ips_v1_no_annotation.ipynb Cell 38'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HYPC300/OneDrive%20-%20%ED%95%9C%EC%96%91%EB%8C%80%ED%95%99%EA%B5%90/GitHub/Deeplearning-Autoencoder-DOA/%5B3%5DPytorch/%5B3%5DTransformer_DOA/transformer_ips_v1_no_annotation.ipynb#ch0000027?line=5'>6</a>\u001b[0m one_trajectory \u001b[39m=\u001b[39m rearrange(test_input[trajectory_ind], \u001b[39m'\u001b[39m\u001b[39ma b -> a 1 b\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HYPC300/OneDrive%20-%20%ED%95%9C%EC%96%91%EB%8C%80%ED%95%99%EA%B5%90/GitHub/Deeplearning-Autoencoder-DOA/%5B3%5DPytorch/%5B3%5DTransformer_DOA/transformer_ips_v1_no_annotation.ipynb#ch0000027?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(one_trajectory\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HYPC300/OneDrive%20-%20%ED%95%9C%EC%96%91%EB%8C%80%ED%95%99%EA%B5%90/GitHub/Deeplearning-Autoencoder-DOA/%5B3%5DPytorch/%5B3%5DTransformer_DOA/transformer_ips_v1_no_annotation.ipynb#ch0000027?line=8'>9</a>\u001b[0m predicted \u001b[39m=\u001b[39m predict1(best_model, start_point, one_trajectory) \u001b[39m# new_model best_model \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HYPC300/OneDrive%20-%20%ED%95%9C%EC%96%91%EB%8C%80%ED%95%99%EA%B5%90/GitHub/Deeplearning-Autoencoder-DOA/%5B3%5DPytorch/%5B3%5DTransformer_DOA/transformer_ips_v1_no_annotation.ipynb#ch0000027?line=9'>10</a>\u001b[0m \u001b[39m# test_ppl = math.exp(test_loss)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HYPC300/OneDrive%20-%20%ED%95%9C%EC%96%91%EB%8C%80%ED%95%99%EA%B5%90/GitHub/Deeplearning-Autoencoder-DOA/%5B3%5DPytorch/%5B3%5DTransformer_DOA/transformer_ips_v1_no_annotation.ipynb#ch0000027?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m89\u001b[39m)\n",
            "\u001b[1;32mc:\\Users\\HYPC300\\OneDrive - 한양대학교\\GitHub\\Deeplearning-Autoencoder-DOA\\[3]Pytorch\\[3]Transformer_DOA\\transformer_ips_v1_no_annotation.ipynb Cell 34'\u001b[0m in \u001b[0;36mpredict1\u001b[1;34m(model, start_point, eval_data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HYPC300/OneDrive%20-%20%ED%95%9C%EC%96%91%EB%8C%80%ED%95%99%EA%B5%90/GitHub/Deeplearning-Autoencoder-DOA/%5B3%5DPytorch/%5B3%5DTransformer_DOA/transformer_ips_v1_no_annotation.ipynb#ch0000023?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_size \u001b[39m!=\u001b[39m n_pts_once:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HYPC300/OneDrive%20-%20%ED%95%9C%EC%96%91%EB%8C%80%ED%95%99%EA%B5%90/GitHub/Deeplearning-Autoencoder-DOA/%5B3%5DPytorch/%5B3%5DTransformer_DOA/transformer_ips_v1_no_annotation.ipynb#ch0000023?line=19'>20</a>\u001b[0m       src_mask \u001b[39m=\u001b[39m src_mask[:batch_size, :batch_size]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HYPC300/OneDrive%20-%20%ED%95%9C%EC%96%91%EB%8C%80%ED%95%99%EA%B5%90/GitHub/Deeplearning-Autoencoder-DOA/%5B3%5DPytorch/%5B3%5DTransformer_DOA/transformer_ips_v1_no_annotation.ipynb#ch0000023?line=20'>21</a>\u001b[0m output \u001b[39m=\u001b[39m model(data, src_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HYPC300/OneDrive%20-%20%ED%95%9C%EC%96%91%EB%8C%80%ED%95%99%EA%B5%90/GitHub/Deeplearning-Autoencoder-DOA/%5B3%5DPytorch/%5B3%5DTransformer_DOA/transformer_ips_v1_no_annotation.ipynb#ch0000023?line=21'>22</a>\u001b[0m output_flat \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, ntokens)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HYPC300/OneDrive%20-%20%ED%95%9C%EC%96%91%EB%8C%80%ED%95%99%EA%B5%90/GitHub/Deeplearning-Autoencoder-DOA/%5B3%5DPytorch/%5B3%5DTransformer_DOA/transformer_ips_v1_no_annotation.ipynb#ch0000023?line=23'>24</a>\u001b[0m       \u001b[39m# total_loss += batch_size * criterion(output_flat, targets).item()\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\pt1.11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/HYPC300/anaconda3/envs/pt1.11/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/HYPC300/anaconda3/envs/pt1.11/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/HYPC300/anaconda3/envs/pt1.11/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/HYPC300/anaconda3/envs/pt1.11/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/HYPC300/anaconda3/envs/pt1.11/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/HYPC300/anaconda3/envs/pt1.11/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/HYPC300/anaconda3/envs/pt1.11/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'src_mask'"
          ]
        }
      ],
      "source": [
        "trajectory_ind = 0 #NOTE: the index of the trajectory from all 49 trajectories.\n",
        "start_point = 0  # 0, n_pts_once, 2*n_pts_once... #NOTE: the start point in a given trajectory\n",
        "\n",
        "test_input = rearrange(test_data[0], 'a b c -> b a c') # train_data test_data\n",
        "print(test_input.shape)\n",
        "one_trajectory = rearrange(test_input[trajectory_ind], 'a b -> a 1 b') \n",
        "print(one_trajectory.shape)\n",
        "\n",
        "predicted = predict1(best_model, start_point, one_trajectory) # new_model best_model \n",
        "# test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "# print(predicted.shape)\n",
        "pre_ind = torch.argmax(predicted,axis=1)\n",
        "# print(pre_ind.reshape(n_pts_once,-1))\n",
        "print(f'the predicted locations are {pre_ind.reshape(len(predicted),-1).squeeze()}')\n",
        "# print(labels_ips.)\n",
        "\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Scenario 2: \n",
        "1. the test data is different from the training data. \n",
        "2. For example, the batch_size is 20, one of the 20 data, only the first one is valid, the remaining ones will be 0, just to conform with the batch_size during the training.\n",
        "\n",
        "`Result`: The result is 100% accurate if the same data is put into the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 414,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict2(model: nn.Module, start_point: Tensor, data_length: Tensor, eval_data: Tensor, eval_data_doa:Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(n_pts_once).to(device)\n",
        "    with torch.no_grad():\n",
        "      if data_length > n_pts_once:\n",
        "          raise ValueError(f\"the data_length should not be greater than the batch_size {n_pts_once}\")\n",
        "      else:\n",
        "        #   if start_point+data_length < 80:\n",
        "        data = eval_data[start_point:start_point+data_length]\n",
        "        data_doa = eval_data_doa[start_point:start_point+data_length]\n",
        "        #   else:\n",
        "        #       data = eval_data[start_point:]\n",
        "        batch_size = data.size(0)\n",
        "        # print(batch_size,'a')\n",
        "        if batch_size != n_pts_once:\n",
        "                src_mask = src_mask[:batch_size, :batch_size]\n",
        "        output = model(data, data_doa,src_mask)\n",
        "        output_flat = output.view(-1, ntokens)\n",
        "    return output_flat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([49, 79, 384])\n",
            "torch.Size([49, 79])\n",
            "torch.Size([79, 1, 384])\n",
            "torch.Size([79, 1])\n",
            "=========================================================================================\n",
            "the predicted locations are tensor([ 560,  209,  209,  209,  560,  560,  209,  209,  209, 2568, 2568,  560,\n",
            "         560, 2568, 2568, 2568, 2568,  560,  560,  560,  560, 1585, 1585,  560,\n",
            "         560,  209,  209,  209,  209,  560,  560,  560,  560,  560,  209,  209,\n",
            "         209,  209,  209,  560], device='cuda:0')\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "trajectory_ind = 0 #NOTE: the index of the trajectory from all 49 trajectories.\n",
        "start_point = 0 # 0, n_pts_once, 2*n_pts_once... #NOTE: the start point in a given trajectory\n",
        "data_length = 40\n",
        "test_data_one = test_data[2]\n",
        "\n",
        "test_input = rearrange(test_data_one.to(device), 'a b c -> b a c') # train_data test_data\n",
        "print(test_input.shape)\n",
        "\n",
        "test_input_doa = rearrange(train_data_doa.to(device), 'a b -> b a') # train_data test_data\n",
        "print(test_input_doa.shape)\n",
        "\n",
        "# print(test_input.shape)\n",
        "one_trajectory = rearrange(test_input[trajectory_ind], 'a b -> a 1 b') \n",
        "one_trajectory_doa = rearrange(test_input_doa[trajectory_ind], 'a -> a 1') \n",
        "print(one_trajectory.shape)\n",
        "print(one_trajectory_doa.shape)\n",
        "\n",
        "\n",
        "predicted = predict2(new_model, start_point, data_length, one_trajectory,one_trajectory_doa)\n",
        "# test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "# print(predicted.shape)\n",
        "pre_ind = torch.argmax(predicted,axis=1)\n",
        "# print(pre_ind.reshape(n_pts_once,-1))\n",
        "print(f'the predicted locations are {pre_ind.reshape(len(predicted),-1).squeeze()}')\n",
        "# print(labels_ips.)\n",
        "\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Scenario 3: \n",
        "Calculate the overall average errors by applying the error reference table\n",
        "NOTE: The average error is calculated across all the positions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualization of the errors in each trajectory as well as the average error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 415,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_traj_error(snr, trj_error):\n",
        "    names = [str(i+1) for i in range(len(trj_error))]\n",
        "    values = [i[0] for i in trj_error]\n",
        "    # the average value\n",
        "    ave_error = sum(values)/len(values)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(15, 5))\n",
        "    ax.bar(names,values)\n",
        "    ax.axhline(ave_error, color='k') # horizontal\n",
        "\n",
        "    ax.annotate(\n",
        "        f'Average: {ave_error:.2f} cm',\n",
        "        xy=(27, ave_error), xycoords='data', size=15,\n",
        "        xytext=(-90, 50), textcoords='offset points',\n",
        "        arrowprops=dict(arrowstyle=\"->\",\n",
        "                        connectionstyle=\"arc,angleA=0,armA=50,rad=10\",\n",
        "                        color=\"k\"))\n",
        "    plt.suptitle(f'Positioning Error of Each Trajectory (SNR = {snr}dB)',size=20)\n",
        "    plt.xlabel('# Trajectory',size=15)\n",
        "    plt.ylabel('Error (cm)',size=15)\n",
        "    plt.savefig(f'{snr}snr_train_on_20_snr_with_doa_100epoch.png')\n",
        "    plt.close(fig)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {},
      "outputs": [],
      "source": [
        "error_snr_no_doa = error_snr;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the errors across all the positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 495,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SNR = -10dB, average error is 11.961143134138677cm\n",
            "SNR = -9dB, average error is 11.961143134138682cm\n",
            "SNR = -8dB, average error is 11.961143134138682cm\n",
            "SNR = -7dB, average error is 11.96114313413868cm\n",
            "SNR = -6dB, average error is 11.961143134138677cm\n",
            "SNR = -5dB, average error is 11.961143134138675cm\n",
            "SNR = -4dB, average error is 11.961143134138673cm\n",
            "SNR = -3dB, average error is 11.96114313413868cm\n",
            "SNR = -2dB, average error is 11.961117890192812cm\n",
            "SNR = -1dB, average error is 11.960408324780612cm\n"
          ]
        }
      ],
      "source": [
        "table = np.arange(0, 79 * 49)\n",
        "table = table.reshape(49, 79)\n",
        "spacing = 10 # 10 cm between each point\n",
        "\n",
        "# calculate the overall positioning error\n",
        "data_length = None\n",
        "trj_error = []\n",
        "# error_snr = dict()\n",
        "# NOTE: to test the data generated under all the SNR (from -10dB to 30dB)\n",
        "for snr in range(-10,0):\n",
        "    test_data_snr = test_data[snr+10] # 0 is snr -10 dB, 40 is snr 30 dB, 30 is 20dB, 20 is 10dB 15 is 5 dB 10 is 0dB 5 is -5dB\n",
        "    for trj in range(49): #note: 49 is the number of trajectories\n",
        "        test_input = rearrange(test_data_snr, 'a b c -> b a c') # train_data test_data\n",
        "        test_input_doa = rearrange(train_data_doa.to(device), 'a b -> b a') # train_data test_data\n",
        "\n",
        "        one_trajectory = rearrange(test_input[trj], 'a b -> a 1 b') \n",
        "        one_trajectory_doa = rearrange(test_input_doa[trj], 'a -> a') \n",
        "        all_error = 0\n",
        "        for str_point in range(79-1):  #note: 79 is the number of points in each trajectory\n",
        "\n",
        "            predicted = predict2(best_model, str_point, n_pts_once, one_trajectory ,one_trajectory_doa) # new_model\n",
        "              \n",
        "            # predicted = predict2(new_model, str_point, n_pts_once, one_trajectory)\n",
        "            pre_ind = torch.argmax(predicted,axis=1)\n",
        "            # print(len(pre_ind),'b')\n",
        "            err_trj = calculate_error(table, trj, str_point, pre_ind, spacing)\n",
        "            all_error = all_error + err_trj\n",
        "            # print(err_trj)\n",
        "        trj_error.append(all_error/78)\n",
        "    values = [i[0] for i in trj_error]\n",
        "    # the average value\n",
        "    ave_error = sum(values)/len(values)\n",
        "    print(f'SNR = {snr}dB, average error is {ave_error}cm')\n",
        "    error_snr[snr] = ave_error\n",
        "    # plot and save the figure\n",
        "    # plot_traj_error(snr, trj_error)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the errors based on SNR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 497,
      "metadata": {},
      "outputs": [],
      "source": [
        "error_snr_withDOA = [11.961143134138677,\n",
        "11.961143134138682,\n",
        "11.961143134138682,\n",
        "11.96114313413868,\n",
        "11.961143134138677,\n",
        "11.961143134138675,\n",
        "11.961143134138673,\n",
        "11.96114313413868,\n",
        "11.961117890192812,\n",
        "11.960408324780612,\n",
        "11.913337765814694,\n",
        "11.833442485541887,\n",
        "11.691940741785533,\n",
        "11.508321262775388,\n",
        "11.232074927021413,\n",
        "10.966924682058615,\n",
        "10.758638022697971,\n",
        "10.591753710448339,\n",
        "10.453197031274007,\n",
        "10.336467100197485,\n",
        "10.23971543291192,\n",
        "10.158290129106595,\n",
        "10.089486670090409,\n",
        "10.029971799035957,\n",
        "9.97850032359246,\n",
        "9.934148869108965,\n",
        "9.894463581570045,\n",
        "9.858787049646615,\n",
        "9.826936877837532,\n",
        "9.798474314812347,\n",
        "9.77241820808085,\n",
        "9.748975347481183,\n",
        "9.727286350826676,\n",
        "9.707512706871128,\n",
        "9.68954436590737,\n",
        "9.673289870813424,\n",
        "9.657871267076334,\n",
        "9.643576704507558,\n",
        "9.630197743418245,\n",
        "9.617907146074646,\n",
        "9.606227302757468]\n",
        "error_snr_withoutDOA = [290.7628089254208,\n",
        "290.7628089254208,\n",
        "290.7628089254207,\n",
        "290.7628089254206,\n",
        "290.76280892542064,\n",
        "290.7628089254207,\n",
        "290.76280892542076,\n",
        "290.7628089254208,\n",
        "290.7621112791757,\n",
        "290.56509611148397,\n",
        "290.02624824035615,\n",
        "289.4126485223524,\n",
        "287.81348009725065,\n",
        "287.33056752759063,\n",
        "286.09976396830996,\n",
        "283.52760812115247,\n",
        "280.1168857056598,\n",
        "275.75506138794077,\n",
        "270.81857785664306,\n",
        "265.6840747632065,\n",
        "260.6965537163995,\n",
        "255.97519862106762,\n",
        "251.53136354677372,\n",
        "247.42891935191938,\n",
        "243.65650200764182,\n",
        "240.18452731721783,\n",
        "236.981985620681,\n",
        "234.0360015498321,\n",
        "231.30583584537757,\n",
        "228.7753514949769,\n",
        "226.42295092463726,\n",
        "224.2302487245853,\n",
        "222.1848179598548,\n",
        "220.2768979950747,\n",
        "218.4840351287932,\n",
        "216.80061929851598,\n",
        "215.22224063987625,\n",
        "213.73046910135827,\n",
        "212.32083055845308,\n",
        "210.98531019945162,\n",
        "209.72350357579705]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "error_snr_20 = [265.2960669,\n",
        "265.2960669,\n",
        "265.2960669,\n",
        "265.2960669,\n",
        "265.2960669,\n",
        "265.2960669,\n",
        "265.2960669,\n",
        "265.2960669,\n",
        "265.2261461,\n",
        "264.6108355,\n",
        "270.136278,\n",
        "314.6082377,\n",
        "333.3966625,\n",
        "320.0825991,\n",
        "270.8881627,\n",
        "219.244125,\n",
        "175.6577249,\n",
        "128.269264,\n",
        "90.81045046,\n",
        "60.26615084,\n",
        "38.61689962,\n",
        "23.27780737,\n",
        "13.13905781,\n",
        "7.514934887,\n",
        "4.482336969,\n",
        "2.794334644,\n",
        "1.81388114,\n",
        "1.361596767,\n",
        "1.112289487,\n",
        "0.9582474997,\n",
        "0.9080324687,\n",
        "0.8603478845,\n",
        "0.8407060223,\n",
        "0.7837711584,\n",
        "0.7174819838,\n",
        "0.7121947389,\n",
        "0.6954038997,\n",
        "0.6727326233,\n",
        "0.674547542,\n",
        "0.6607467632,\n",
        "0.659466191]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "error_snr_10db = error_snr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 499,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(list(range(-10,31)),error_snr_withDOA, label='Training with the DOA embedding')\n",
        "plt.plot(list(range(-10,31)),error_snr_withoutDOA,label='Training without the DOA embedding ')\n",
        "# plt.plot(list(range(-10,31)),error_snr_allSNR,label='Training on the data of all SNR ')\n",
        "plt.title(f'Positioning Error with and without DOA embedding (100epochs)',size=20)\n",
        "plt.xlabel('SNR (dB)',size=15)\n",
        "plt.ylabel('Error (cm)',size=15)\n",
        "plt.grid()\n",
        "plt.legend(title_fontsize=100)\n",
        "plt.savefig(f'DOA Embedding.png')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = ['group_a', 'group_b', 'group_c']\n",
        "values = [1, 10, 100]\n",
        "\n",
        "plt.figure(figsize=(9, 3))\n",
        "\n",
        "plt.bar(names, values)\n",
        "plt.suptitle('Categorical Plotting')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Scenario 3: \n",
        "1. the test data is different from the training data. The test data is generated with three different SNR. \n",
        "2. For example, the batch_size is 20, one of the 20 data, only the first one is valid, the remaining ones will be 0, just to conform with the batch_size during the training.\n",
        "\n",
        "`Result`: The result is 100% accurate if the same data is put into the model. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
